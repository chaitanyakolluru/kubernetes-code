core concepts:

	kubernetes cluster architecture:
		control plane - manager
			- api server
				communciation hub for all cluster components.
			- scheduler
				asiigns an app and pods within an app to a worker node
			- controller manager
				maintains clsuter
			- etcd
				data store storing cluster confniguration
		worker node
			- kubelet
				runs and amanges containers - talks to api server and to container runtime.
			- kube-proxy
				load balances traffic bw app components
			- container runtime 
				docker, rkt, contrainerd etc.
		
		kubernetes has a declarative intent.

		cat << EOF | kubectl create -f -
		apiVersion: v1
		kind: Pod
		metadata:
		  name: nginx
		spec:
		  containers:
		  - name: nginx
			image: nginx
		EOF
	
	$ kc get pods -o wide
	NAME      READY   STATUS    RESTARTS   AGE     IP            NODE                              NOMINATED NODE   READINESS GATES
	busybox   1/1     Running   0          7m12s   10.200.0.2    chaitanyah3684c.mylabserver.com   <none>           <none>
	curlpod   1/1     Running   0          4m49s   10.200.0.10   chaitanyah3683c.mylabserver.com   <none>           <none>
	nginx     1/1     Running   0          15s     10.200.0.8    chaitanyah3683c.mylabserver.com   <none>           <none>
	cloud_user@chaitanyah3685c:~/kubernetes-code$ kc exec -it curlpod -- curl --head 10.200.0.8
	HTTP/1.1 200 OK
	Server: nginx/1.17.8
	Date: Sat, 22 Feb 2020 02:27:16 GMT
	Content-Type: text/html
	Content-Length: 612
	Last-Modified: Tue, 21 Jan 2020 13:36:08 GMT
	Connection: keep-alive
	ETag: "5e26fe48-264"
	Accept-Ranges: bytes

	
	api primitives:
		cloud_user@chaitanyah3681c:~$ kubectl get componentstatus
		NAME                 STATUS    MESSAGE              ERROR
		controller-manager   Healthy   ok
		scheduler            Healthy   ok
		etcd-0               Healthy   {"health": "true"}

		kubectl get deployments nginx-deployment -o wide
		kubectl get deployments nginx-deployment -o yaml

		$ kc get deploy

		$ kc get pods
		NAME                                READY   STATUS    RESTARTS   AGE
		busybox                             1/1     Running   0          16m
		curlpod                             1/1     Running   0          13m
		nginx-deployment-5f7df8d587-h4dj4   1/1     Running   0          2m23s
		cloud_user@chaitanyah3685c:~/kubernetes-code$ kc get svc
		NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
		kubernetes      ClusterIP   10.32.0.1     <none>        443/TCP   9d
		nginx-service   ClusterIP   10.32.0.250   <none>        80/TCP    30s
		cloud_user@chaitanyah3685c:~/kubernetes-code$
		cloud_user@chaitanyah3685c:~/kubernetes-code$
		cloud_user@chaitanyah3685c:~/kubernetes-code$ kc exec -it curlpod -- curl --head 10.32.0.250
		HTTP/1.1 200 OK
		Server: nginx/1.17.8
		Date: Sat, 22 Feb 2020 02:36:23 GMT
		Content-Type: text/html
		Content-Length: 612
		Last-Modified: Tue, 21 Jan 2020 13:36:08 GMT
		Connection: keep-alive
		ETag: "5e26fe48-264"
		Accept-Ranges: bytes


		kubectl get pods --show-labels

		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl get pods --show-labels
		NAME                                READY   STATUS    RESTARTS   AGE     LABELS
		nginx-deployment-5c689d88bb-2ddg5   1/1     Running   0          7m51s   app=nginx,pod-template-hash=5c689d88bb
		nginx-deployment-5c689d88bb-m4gnw   1/1     Running   0          7m51s   app=nginx,pod-template-hash=5c689d88bb
		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl label deployments nginx-deployment env=prod
		deployment.extensions/nginx-deployment labeled
		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl get deployments
		NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   2         2         2            2           9m5s
		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl get deployments --show-labels
		NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE     LABELS
		nginx-deployment   2         2         2            2           9m10s   env=prod
		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl get pods -L env
		NAME                                READY   STATUS    RESTARTS   AGE     ENV
		nginx-deployment-5c689d88bb-2ddg5   1/1     Running   0          9m21s
		nginx-deployment-5c689d88bb-m4gnw   1/1     Running   0          9m21s
		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl annotate deployments nginx-deployment mycompany.com/tempannotation="chad"
		deployment.extensions/nginx-deployment annotated
		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl get pods --field-selector=status.phase=Running
		NAME                                READY   STATUS    RESTARTS   AGE
		nginx-deployment-5c689d88bb-2ddg5   1/1     Running   0          10m
		nginx-deployment-5c689d88bb-m4gnw   1/1     Running   0          10m
		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl get pods --field-selector=status.phase=Running,metadata.namespace=default
		NAME                                READY   STATUS    RESTARTS   AGE
		nginx-deployment-5c689d88bb-2ddg5   1/1     Running   0          10m
		nginx-deployment-5c689d88bb-m4gnw   1/1     Running   0          10m


		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl get pods --field-selector=status.phase==Running,metadata.namespace==default
		NAME                                READY   STATUS    RESTARTS   AGE
		nginx-deployment-5c689d88bb-2ddg5   1/1     Running   0          11m
		nginx-deployment-5c689d88bb-m4gnw   1/1     Running   0          11m
		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl get pods --field-selector=status.phase==Running,metadata.namespace!=default
		No resources found.

				
	kubernetes services and network primitives:

		apiVersion: v1
		kind: Service
		metadata:
		  name: nginx-nodeport
		spec:
		  type: NodePort
		  ports:
		  - protocol: TCP
			port: 80
			targetPort: 80
			nodePort: 30080
		  selector:
			app: nginx
			
	$ kc explain svc.spec.type
	KIND:     Service
	VERSION:  v1

	FIELD:    type <string>

	DESCRIPTION:
		 type determines how the Service is exposed. Defaults to ClusterIP. Valid
		 options are ExternalName, ClusterIP, NodePort, and LoadBalancer.
		 "ExternalName" maps to the specified externalName. "ClusterIP" allocates a
		 cluster-internal IP address for load-balancing to endpoints. Endpoints are
		 determined by the selector or if that is not specified, by manual
		 construction of an Endpoints object. If clusterIP is "None", no virtual IP
		 is allocated and the endpoints are published as a set of endpoints rather
		 than a stable IP. "NodePort" builds on ClusterIP and allocates a port on
		 every node which routes to the clusterIP. "LoadBalancer" builds on NodePort
		 and creates an external load-balancer (if supported in the current cloud)
		 which routes to the clusterIP. More info:
		 https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
	cloud_user@chaitanyah3685c:~/kubernetes-code$ kc get ep
	NAME            ENDPOINTS                               AGE
	kubernetes      172.31.118.87:6443,172.31.119.25:6443   9d
	nginx-service   10.200.0.8:80                           5m57s


		To create the busybox pod to run commands from:

		cat << EOF | kubectl create -f -
		
		apiVersion: v1
		kind: Pod
		metadata:
		  name: busybox
		spec:
		  containers:
		  - name: busybox
			image: radial/busyboxplus:curl
			args:
			- sleep
			- "1000"
		EOF

		$ kubectl run busybox --generator=run-pod/v1 --image=radial/busyboxplus:curl -- sleep 1000


		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl get pods -o wide
		NAME                                READY   STATUS    RESTARTS   AGE   IP            NODE                              NOMINATED NODE
		busybox                             1/1     Running   0          75s   10.244.2.21   chaitanyah3683c.mylabserver.com   <none>
		nginx-deployment-5c689d88bb-2ddg5   1/1     Running   0          22m   10.244.2.20   chaitanyah3683c.mylabserver.com   <none>
		nginx-deployment-5c689d88bb-m4gnw   1/1     Running   0          22m   10.244.1.20   chaitanyah3682c.mylabserver.com   <none>
		cloud_user@chaitanyah3681c:~/cert-kube$
		cloud_user@chaitanyah3681c:~/cert-kube$
		cloud_user@chaitanyah3681c:~/cert-kube$
		cloud_user@chaitanyah3681c:~/cert-kube$
		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl get services
		NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
		kubernetes       ClusterIP   10.96.0.1        <none>        443/TCP        24h
		nginx-nodeport   NodePort    10.111.199.152   <none>        80:30080/TCP   6m22s
		cloud_user@chaitanyah3681c:~/cert-kube$
		cloud_user@chaitanyah3681c:~/cert-kube$
		cloud_user@chaitanyah3681c:~/cert-kube$
		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl exec busybox -- curl 10.111.199.152:80
		  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
										 Dload  Upload   Total   Spent    Left  Speed
		100   612  100   612    0     0   158k      0 --:--:-- --:--:-- --:--:--  199k
		<!DOCTYPE html>
		<html>
		<head>
		<title>Welcome to nginx!</title>
		<style>
			body {
				width: 35em;
				margin: 0 auto;
				font-family: Tahoma, Verdana, Arial, sans-serif;
			}
		</style>
		</head>
		<body>
		<h1>Welcome to nginx!</h1>
		<p>If you see this page, the nginx web server is successfully installed and
		working. Further configuration is required.</p>

		<p>For online documentation and support please refer to
		<a href="http://nginx.org/">nginx.org</a>.<br/>
		Commercial support is available at
		<a href="http://nginx.com/">nginx.com</a>.</p>

		<p><em>Thank you for using nginx.</em></p>
		</body>
		</html>

install, config and validate:

	install master and nodes:
		Get the Docker gpg key:

		curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

		Add the Docker repository:

		sudo add-apt-repository    "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
		   $(lsb_release -cs) \
		   stable"

		Get the Kubernetes gpg key:

		curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

		Add the Kubernetes repository:

		cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
		deb https://apt.kubernetes.io/ kubernetes-xenial main
		EOF

		Update your packages:

		sudo apt-get update

		Install Docker, kubelet, kubeadm, and kubectl:

		sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu kubelet=1.13.5-00 kubeadm=1.13.5-00 kubectl=1.13.5-00
		sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu kubelet=1.16.0-00 kubeadm=1.16.0-00 kubectl=1.16.0-00
		sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu kubelet=1.19.0-00 kubeadm=1.19.0-00 kubectl=1.19.0-00 --allow-unauthenticated
		
		20.04 focal:
		# cat /etc/issue
		Ubuntu 20.04.1 LTS \n \l
		sudo apt-get install -y docker-ce=5:20.10.3~3-0~ubuntu-focal kubelet=1.20.2-00 kubeadm=1.20.2-00 kubectl=1.20.2-00 --allow-unauthenticated


		Hold them at the current version:

		sudo apt-mark hold docker-ce kubelet kubeadm kubectl

		Add the iptables rule to sysctl.conf:

		echo "net.bridge.bridge-nf-call-iptables=1" | sudo tee -a /etc/sysctl.conf

		Enable iptables immediately:

		sudo sysctl -p

		Initialize the cluster (run only on the master):

		sudo kubeadm init --pod-network-cidr=10.244.0.0/16

		Set up local kubeconfig:

		mkdir -p $HOME/.kube

		sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

		sudo chown $(id -u):$(id -g) $HOME/.kube/config

		** new for 1.17.8-00- calico cni used: 1.18.12-00 being upgraded to at the end.
		kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml
		
		Apply Flannel CNI network overlay:

		**different cni require diff pod-cidr to be sued with kubeadm init --pod-network-cidr.
		also, kubeadm automatically allocates a non overlapping cidr to each of the worker nodes in the cluster.
		kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

		**when creating a cluster using kubeadm;
		all the conf are stored as config maps in kube-system namespace like so:
		$ kubectl get cm --all-namespaces
		NAMESPACE     NAME                                 DATA   AGE
		kube-public   cluster-info                         2      29m
		kube-system   coredns                              1      29m
		kube-system   extension-apiserver-authentication   6      29m
		kube-system   kube-flannel-cfg                     2      15m
		kube-system   kube-proxy                           2      29m
		kube-system   kubeadm-config                       2      29m
		kube-system   kubelet-config-1.13                  1      29m

		**how to create and use new tokens to join nodes to the cluster:
		$ kubeadm token create --print-join-command
		kubeadm join 172.31.101.11:6443 --token qrqkkt.v6r0bx3oly6d8ngh --discovery-token-ca-cert-hash sha256:b82f56a3c53c7e88462c76d1b1b7f835ff7752b43c888aca04d1ca7428be6804
		cloud_user@chaitanyah3681c:~$ kubeadm token list
		TOKEN                     TTL       EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
		i9fipl.h3wi0d046y0gx0cp   23h       2020-01-02T00:05:00Z   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token
		qrqkkt.v6r0bx3oly6d8ngh   23h       2020-01-02T00:38:23Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token
		cloud_user@chaitanyah3681c:~$

		**kubeadm join 172.31.101.11:6443 --token qrqkkt.v6r0bx3oly6d8ngh --discovery-token-ca-cert-hash sha256:b82f56a3c53c7e88462c76d1b1b7f835ff7752b43c888aca04d1ca7428be6804
		command to join a worker node to a cluster needs to be run as root.
		
		Join the worker nodes to the cluster:

		kubeadm join [your unique string from the kubeadm init command]

		Verify the worker nodes have joined the cluster successfully:

		kubectl get nodes

		Compare this result of the kubectl get nodes command:

		NAME                            STATUS   ROLES    AGE   VERSION
		chadcrowell1c.mylabserver.com   Ready    master   4m18s v1.13.5
		chadcrowell2c.mylabserver.com   Ready    none     82s   v1.13.5
		chadcrowell3c.mylabserver.com   Ready    none     69s   v1.13.5

		**everything kubeadm init does:
		The "init" command executes the following phases:
		```
		preflight                  Run master pre-flight checks
		kubelet-start              Writes kubelet settings and (re)starts the kubelet
		certs                      Certificate generation
		  /etcd-ca                   Generates the self-signed CA to provision identities for etcd
		  /apiserver-etcd-client     Generates the client apiserver uses to access etcd
		  /etcd-healthcheck-client   Generates the client certificate for liveness probes to healtcheck etcd
		  /etcd-server               Generates the certificate for serving etcd
		  /etcd-peer                 Generates the credentials for etcd nodes to communicate with each other
		  /front-proxy-ca            Generates the self-signed CA to provision identities for front proxy
		  /front-proxy-client        Generates the client for the front proxy
		  /ca                        Generates the self-signed Kubernetes CA to provision identities for other Kubernetes components
		  /apiserver                 Generates the certificate for serving the Kubernetes API
		  /apiserver-kubelet-client  Generates the Client certificate for the API server to connect to kubelet
		  /sa                        Generates a private key for signing service account tokens along with its public key
		kubeconfig                 Generates all kubeconfig files necessary to establish the control plane and the admin kubeconfig file
		  /admin                     Generates a kubeconfig file for the admin to use and for kubeadm itself
		  /kubelet                   Generates a kubeconfig file for the kubelet to use *only* for cluster bootstrapping purposes
		  /controller-manager        Generates a kubeconfig file for the controller manager to use
		  /scheduler                 Generates a kubeconfig file for the scheduler to use
		control-plane              Generates all static Pod manifest files necessary to establish the control plane
		  /apiserver                 Generates the kube-apiserver static Pod manifest
		  /controller-manager        Generates the kube-controller-manager static Pod manifest
		  /scheduler                 Generates the kube-scheduler static Pod manifest
		etcd                       Generates static Pod manifest file for local etcd.
		  /local                     Generates the static Pod manifest file for a local, single-node local etcd instance.
		upload-config              Uploads the kubeadm and kubelet configuration to a ConfigMap
		  /kubeadm                   Uploads the kubeadm ClusterConfiguration to a ConfigMap
		  /kubelet                   Uploads the kubelet component config to a ConfigMap
		mark-control-plane         Mark a node as a control-plane
		bootstrap-token            Generates bootstrap tokens used to join a node to a cluster
		addon                      Installs required addons for passing Conformance tests
		  /coredns                   Installs the CoreDNS addon to a Kubernetes cluster
		  /kube-proxy                Installs the kube-proxy addon to a Kubernetes cluster
		```

	high availablity and fault tolerance:

		cloud_user@chaitanyah3681c:~/cert-kube$ kubectl get endpoints kube-scheduler -n kube-system -o yaml
		apiVersion: v1
		kind: Endpoints
		metadata:
		  annotations:
			control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"chaitanyah3681c.mylabserver.com_a219b562-6f65-11e9-9969-0ae96c07b466","leaseDurationSeconds":15,"acquireTime":"2019-05-05T18:43:58Z","renewTime":"2019-05-05T20:20:43Z","leaderTransitions":1}'
		  creationTimestamp: 2019-05-04T19:28:45Z
		  name: kube-scheduler
		  namespace: kube-system
		  resourceVersion: "41221"
		  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler
		  uid: d55b2153-6ea2-11e9-ab2f-0ae96c07b466

		  
		  holds the identity of the leader: holderIdentity
		  
		View the pods in the default namespace with a custom view:

		kubectl get pods -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system

		View the kube-scheduler YAML:

		kubectl get endpoints kube-scheduler -n kube-system -o yaml

		Create a stacked etcd topology using kubeadm:

		kubeadm init --config=kubeadm-config.yaml

		Watch as pods are created in the default namespace:

		kubectl get pods -n kube-system -w


	securing clsuter communciations:
		cat .kube/config | more

		View the service account token:

		kubectl get secrets

		Create a new namespace named my-ns:

		kubectl create ns my-ns

		Run the kube-proxy pod in the my-ns namespace:

		kubectl run test --image=chadmcrowell/kubectl-proxy -n my-ns

		List the pods in the my-ns namespace:

		kubectl get pods -n my-ns

		Run a shell in the newly created pod:

		kubectl exec -it <name-of-pod> -n my-ns sh

		List the services in the namespace via API call:

		curl localhost:8001/api/v1/namespaces/my-ns/services

		View the token file from within a pod:

		cat /var/run/secrets/kubernetes.io/serviceaccount/token

		List the service account resources in your cluster:
		
		coz myns/default sa doesnt have permissions to do so.
		let me add a service read role on that and then::
		
		$ kc create role myns-service-read -n myns --verb=get,list --resource=service
		role.rbac.authorization.k8s.io/myns-service-read created

		$ kc create rolebinding myns-serivce-read-myns-default -n myns --role=myns-service-read --serviceaccount=myns:default
		rolebinding.rbac.authorization.k8s.io/myns-serivce-read-myns-default created

		now, it works::
		 # curl localhost:8001/api/v1/namespaces/myns/services
		{
		  "kind": "ServiceList",
		  "apiVersion": "v1",
		  "metadata": {
			"selfLink": "/api/v1/namespaces/myns/services",
			"resourceVersion": "87523"
		  },
		  "items": []

		kubectl get serviceaccounts


	end to end tests on your cluster:

		Run a simple nginx deployment:

		kubectl run nginx --image=nginx

		View the deployments in your cluster:

		kubectl get deployments

		View the pods in the cluster:

		kubectl get pods

		Use port forwarding to access a pod directly:

		kubectl port-forward $pod_name 8081:80
		
		kubectl port-forward deployment/nginx-deployment 8888:80

		Get a response from the nginx pod directly:

		curl --head http://127.0.0.1:8081

		View the logs from a pod:

		kubectl logs $pod_name

		Run a command directly from the container:

		kubectl exec -it <pod name> -- nginx -v

		Create a service by exposing port 80 of the nginx deployment:

		kubectl expose deployment nginx --port 80 --type NodePort

		List the services in your cluster:

		kubectl get services

		Get a response from the service:

		curl -I localhost:$node_port

		List the nodes' status:

		kubectl get nodes

		View detailed information about the nodes:

		kubectl describe nodes

		View detailed information about the pods:

		kubectl describe pods

		
cluster:

	upgrading the kubernetes cluster:

		View the version of the server and client on the master node:


		kubectl version --short

		View the version of the scheduler and controller manager:

		kubectl get pods -n kube-system kube-controller-manager-chadcrowell1c.mylabserver.com -o yaml

		View the name of the kube-controller pod:

		kubectl get pods -n kube-system

		Set the VERSION variable to the latest stable release of Kubernetes:
		export VERSION=v1.14.1

		Set the ARCH variable to the amd64 system:

		export ARCH=amd64
		

		View the latest stable version of Kubernetes using the variable:

		echo $VERSION

		Curl the latest stable version of Kubernetes:

		curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubeadm > kubeadm

		Install the latest version of kubeadm:

		sudo install -o root -g root -m 0755 ./kubeadm /usr/bin/kubeadm

		Check the version of kubeadm:

		sudo kubeadm version

		Plan the upgrade:

		sudo kubeadm upgrade plan

		Apply the upgrade to 1.14.1:

		kubeadm upgrade apply v1.14.1

		View the differences between the old and new manifests:

		diff kube-controller-manager.yaml /etc/kubernetes/manifests/kube-controller-manager.yaml

		Curl the latest version of kubelet:

		curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubelet > kubelet

		Install the latest version of kubelet:

		sudo install -o root -g root -m 0755 ./kubelet /usr/bin/kubelet

		Restart the kubelet service:

		sudo systemctl restart kubelet.service

		Watch the nodes as they change version:
		
		Curl the latest version of kubelet:

		curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubectl > kubectl

		Install the latest version of kubelet:


		kubectl get nodes -w

		**if u get this message when attempting to upgrade using  kubeadm, then upgrade kubelet first and then attempt the upgrade.
		  - There are kubelets in this cluster that are too old that have these versions [v1.13.5]
		
	performing upgrades to os within a kubernetes cluster:

		See which pods are running on which nodes:

		kubectl get pods -o wide

		Evict the pods on a node:

		kubectl drain [node_name] --ignore-daemonsets

		Watch as the node changes status:

		kubectl get nodes -w

		Schedule pods to the node after maintenance is complete:

		kubectl uncordon [node_name]

		Remove a node from the cluster:

		kubectl delete node [node_name]

		Generate a new token:

		sudo kubeadm token generate

		List the tokens:

		sudo kubeadm token list

		Print the kubeadm join command to join a node to the cluster:

		sudo kubeadm token create [token_name] --ttl 2h --print-join-command


		*node draining/token creation stuff*:
		 kc get no
		NAME                           STATUS   ROLES    AGE   VERSION
		5575e104891c.mylabserver.com   Ready    <none>   68m   v1.18.12
		7e393cfe541c.mylabserver.com   Ready    master   71m   v1.18.12
		faa65cd3621c.mylabserver.com   Ready    <none>   68m   v1.18.12

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-12-7 (master)
		$ kc drain 5575e104891c.mylabserver.com --ignore-daemonsets
		node/5575e104891c.mylabserver.com cordoned
		WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-d89mx, kube-system/kube-proxy-zzbxc
		node/5575e104891c.mylabserver.com drained

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-12-7 (master)
		$ kc get no
		NAME                           STATUS                     ROLES    AGE   VERSION
		5575e104891c.mylabserver.com   Ready,SchedulingDisabled   <none>   69m   v1.18.12
		7e393cfe541c.mylabserver.com   Ready                      master   72m   v1.18.12
		faa65cd3621c.mylabserver.com   Ready                      <none>   69m   v1.18.12

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-12-7 (master)
		$ kc delete node 5575e104891c.mylabserver.com
		node "5575e104891c.mylabserver.com" deleted

		$ kubeadm token list
		TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
		ouwm4l.lxwwbv1ryaqqmwrp   22h         2020-12-09T03:27:02Z   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token
		cloud_user@7e393cfe541c:~/.kube$ kubeadm token generate
		cloud_user@7e393cfe541c:~/.kube$ date
		Tue Dec  8 04:40:52 UTC 2020
		cloud_user@7e393cfe541c:~/.kube$ kubeadm token generate
		xmuxk7.n1y0lfl91fdb7ntv
		cloud_user@7e393cfe541c:~/.kube$ kubeadm token create xmuxk7.n1y0lfl91fdb7ntv --ttl 2h --print-join-command
		W1208 04:41:52.452903    2682 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
		kubeadm join 172.31.99.166:6443 --token xmuxk7.n1y0lfl91fdb7ntv     --discovery-token-ca-cert-hash sha256:486a789182cf85aa3e2fb0399b63afed48448089f3241d678766ecb7a4883320

		and i ran this to ignore pre-flight checks and add a previously existing node to the cluster using a new token command:
		sudo kubeadm join 172.31.99.166:6443 --token xmuxk7.n1y0lfl91fdb7ntv     --discovery-token-ca-cert-hash sha256:486a789182cf85aa3e2fb0399b63afed48448089f3241d678766ecb7a4883320 --ignore-preflight-errors=FileAvailable--etc-kubernetes-kubelet.conf,Port-10250,FileAvailable--etc-kubernetes-pki-ca.crt

		$ kubectl get node
		NAME                           STATUS     ROLES    AGE   VERSION
		5575e104891c.mylabserver.com   NotReady   <none>   4s    v1.18.12
		7e393cfe541c.mylabserver.com   Ready      master   77m   v1.18.12
		faa65cd3621c.mylabserver.com   Ready      <none>   74m   v1.18.12


	backing up and restoring cluster:
		backing up etcd:

		Get the etcd binaries:

		wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz

		Unzip the compressed binaries:

		tar xvf etcd-v3.3.12-linux-amd64.tar.gz

		Move the files into /usr/local/bin:

		sudo mv etcd-v3.3.12-linux-amd64/etcd* /usr/local/bin

		Take a snapshot of the etcd datastore using etcdctl:

		sudo ETCDCTL_API=3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/server.crt --cert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/ca.key

		View the help page for etcdctl:

		ETCDCTL_API=3 etcdctl --help

		Browse to the folder that contains the certificate files:

		cd /etc/kubernetes/pki/etcd/

		View that the snapshot was successful:

		ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db

		Zip up the contents of the etcd directory:

		**the cert directory
		sudo tar -zcvf etcd.tar.gz /etc/kubernetes/pki/etcd

		Copy the etcd directory to another server:

		scp etcd.tar.gz cloud_user@18.219.235.42:~/

		so the etcd dir and the snapshot.db and restore using the etcdctl snapshot restore command.

networking:

	pod and node networking:

		See which node our pod is on:

		kubectl get pods -o wide

		Log in to the node:

		ssh [node_name]

		View the node's virtual network interfaces:

		ifconfig

		View the containers in the pod:

		docker ps

		Get the process ID for the container:

		docker inspect --format '{{ .State.Pid }}' [container_id]

		Use nsenter to run a command in the process's network namespace:

		nsenter -t [container_pid] -n ip addr
			sudo nsenter -t 24690 -n ip addr
			1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
				link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
				inet 127.0.0.1/8 scope host lo
				valid_lft forever preferred_lft forever
			2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
				link/ipip 0.0.0.0 brd 0.0.0.0
			4: eth0@if10: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP group default
				link/ether 1a:4f:d1:9c:10:0a brd ff:ff:ff:ff:ff:ff link-netnsid 0
				inet 10.244.219.4/32 scope global eth0
				valid_lft forever preferred_lft forever

			** 4: eth0@if10 here means the eth0 and if10 are a virtual pair and that if10 corresponds to 10th interface on ip a command op:
			10: cali4eb4bc0a4fb@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1440 qdisc noqueue state UP group default
			link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1
			inet6 fe80::ecee:eeff:feee:eeee/64 scope link
			valid_lft forever preferred_lft forever


	container network interface:
		is a nw overlay - sort of a tunnel.

		kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml
	
	diagram on how inter-node communication occurs:
		node1								node2
		pod1								pod2
		ip 1								ip 2
		eth0								eth1
		veth0								veth1

		eth0->veth0->bridge->eth0(node1)  ====CNI ======= eth0(node2)->bridge->veth1->eth1

	**the cni used is sort of hinted at by the pod-cidr one uses when kubeadm init is performed. 
	
	$ kubectl get pods -n kube-system -o wide | grep h3681c | awk '{print $1}'
	etcd-chaitanyah3681c.mylabserver.com
	kube-apiserver-chaitanyah3681c.mylabserver.com
	kube-controller-manager-chaitanyah3681c.mylabserver.com
	kube-flannel-ds-amd64-dwdwz
	kube-proxy-ntmhx
	kube-scheduler-chaitanyah3681c.mylabserver.com

	$ kubectl get pods -n kube-system -o wide | grep h3682c | awk '{print $1}'
	coredns-5d4dd4b4db-gpl5x
	kube-flannel-ds-amd64-vw7xp
	kube-proxy-l4md5

	$ kubectl get pods -n kube-system -o wide | grep h3683c | awk '{print $1}'
	coredns-5d4dd4b4db-p7zxv
	kube-flannel-ds-amd64-f24p6
	kube-proxy-p8vtp

	kubeadm creates etcd, apiserver, controller manager, scheduler as pods in kube-system namespace instead of how
	we created them as services on the controllers in hard way guide. 

		
	service networking:

		YAML for the nginx NodePort service:

		apiVersion: v1
		kind: Service
		metadata:
		  name: nginx-nodeport
		spec:
		  type: NodePort
		  ports:
		  - protocol: TCP
			port: 80
			targetPort: 80
			nodePort: 30080
		  selector:
			app: nginx

		Get the services YAML output for all the services in your cluster:

		kubectl get services -o yaml

		Try and ping the clusterIP service IP address:

		ping 10.96.0.1

		View the list of services in your cluster:

		kubectl get services

		View the list of endpoints in your cluster that get created with a service:

		kubectl get endpoints

		Look at the iptables rules for your services:

		sudo iptables-save | grep KUBE | grep nginx


	ingress rules and load balancers:

		View the list of services:

		kubectl get services

		The load balancer YAML spec:

		apiVersion: v1
		kind: Service
		metadata:
		  name: nginx-loadbalancer
		spec:
		  type: LoadBalancer
		  ports:
		  - port: 80
			targetPort: 80
		  selector:
			app: nginx

		Create a new deployment:


		kubectl run kubeserve2 --image=chadmcrowell/kubeserve2

		View the list of deployments:

		kubectl get deployments

		Scale the deployments to 2 replicas:

		kubectl scale deployment/kubeserve2 --replicas=2

		View which pods are on which nodes:

		kubectl get pods -o wide

		Create a load balancer from a deployment:

		kubectl expose deployment kubeserve2 --port 80 --target-port 8080 --type LoadBalancer
		
		kubectl expose deployment kubeserve2 --port 80 --target-port 8080 --container-port 30082 --type LoadBalancer

		
		View the services in your cluster:

		kubectl get services

		Watch as an external ip is created for a service:

		kubectl get services -w

		Look at the YAML for a service:

		kubectl get services kubeserve2 -o yaml

		Curl the external IP of the load balancer:

		curl http://[external-ip]

		View the annotation associated with a service:

		kubectl describe services kubeserve

		Set the annotation to route load balancer traffic local to the node:

		kubectl annotate service kubeserve2 externalTrafficPolicy=Local
		**local vs cluster external traffic policy:
			local - means that traffic is distributed at a node level and not at a cluster level.
			this means, if u have 5 in node1 and 10 pods in node2, it will distribute all requests equally between the two nodes, 
			so the pods in node1 will have to work more because of hving less no of pods in the node1.

		The YAML for an Ingress resource:

		apiVersion: extensions/v1beta1
		kind: Ingress
		metadata:
		  name: name-virtual-host-ingress
		spec:
		  rules:
		  - host: first.bar.com
			http:
			  paths:
			  - backend:
				  serviceName: service1
				  servicePort: 80
		  - host: second.foo.com
			http:
			  paths:
			  - backend:
				  serviceName: service2
				  servicePort: 80
		  - http:
			  paths:
			  - backend:
				  serviceName: service3
				  servicePort: 80

		$ kubectl get ingress
		NAME                 HOSTS         ADDRESS   PORTS   AGE
		kubeserve2-ingress   foo.bar.com             80      8s
		cloud_user@chaitanyah3681c:~/practice-5$
		cloud_user@chaitanyah3681c:~/practice-5$
		cloud_user@chaitanyah3681c:~/practice-5$
		cloud_user@chaitanyah3681c:~/practice-5$
		cloud_user@chaitanyah3681c:~/practice-5$ kubectl describe ingress kubeserve2-ingress
		Name:             kubeserve2-ingress
		Namespace:        default
		Address:
		Default backend:  default-http-backend:80 (<none>)
		Rules:
		  Host         Path  Backends
		  ----         ----  --------
		  foo.bar.com
						  kubeserve2:80 (10.244.1.128:8080,10.244.2.148:8080)
		Annotations:
		  kubectl.kubernetes.io/last-applied-configuration:  {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"name":"kubeserve2-ingress","namespace":"default"},"spec":{"rules":[{"host":"foo.bar.com","http":{"paths":[{"backend":{"serviceName":"kubeserve2","servicePort":80}}]}}]}}

		Events:  <none>

				  
				  
		**ingress explained:
		https://kubernetes.io/docs/concepts/services-networking/ingress/
		
		also, 

		An Ingress is an object that only provides a configuration, not an active component (such as a Pod or a Service). As coreypobrien said, you need to deploy an Ingress controller, which will read the ingresses you deployed in your cluster and change its configuration accordingly.

		At this page you can find the documentation of the official kubernetes ingress controller, based on nginx https://github.com/kubernetes/ingress-nginx/blob/master/README.md

		**more info below:

		$ cat ingress-controller.yaml
		apiVersion: extensions/v1beta1
		kind: Ingress
		metadata:
		  name: ingress-controller
		spec:
		  rules:
		  - http:
			  paths:
			  - path: /nginx
				backend:
				  serviceName: nginx-service
				  servicePort: 80
			  - path: /kubeserve
				backend:
				  serviceName: kubeserve-service
				  servicePort: 80

		**this creates two backends; depending on the path on the request, the request
		goes to a specific service. 
		
		need to also deploy a ingress controller - > kubernetes/ingress-nginx
		
		https://devopscube.com/setup-ingress-kubernetes-nginx-controller/
		
	**new one	kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.35.0/deploy/static/provider/baremetal/deploy.yaml
		
		*****------
		u also have to deploy a cluster ip service ingress-nginx with 80-> 80 and 443-> 443 like so:
		 https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml

		 then connect thru the service using curlpod to access ingress endpoints.
		 
		 apiVersion: v1
		kind: Service
		metadata:
		  name: ingress-nginx
		  namespace: ingress-nginx
		  labels:
			app.kubernetes.io/name: ingress-nginx
			app.kubernetes.io/part-of: ingress-nginx
		spec:
		  type: NodePort
		  ports:
			- name: http
			  port: 80
			  targetPort: 80
			  protocol: TCP
			- name: https
			  port: 443
			  targetPort: 443
			  protocol: TCP
		  selector:
			app.kubernetes.io/name: ingress-nginx
			app.kubernetes.io/part-of: ingress-nginx

		---------***** this is not needed with new one *** (	kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.35.0/deploy/static/provider/baremetal/deploy.yaml)
				******* as this yaml also creates a ingress-nginx-controller in ingress-nginx ns that handles the job that this service does. see below:

		$ kc get svc -n ingress-nginx
		NAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
		ingress-nginx                        NodePort    10.97.190.168    <none>        80:32598/TCP,443:31735/TCP   17h
		ingress-nginx-controller             NodePort    10.110.196.105   <none>        80:31812/TCP,443:32201/TCP   17h
		ingress-nginx-controller-admission   ClusterIP   10.108.59.11     <none>        443/TCP                      17h

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-yamls-practice (master)
		$ kc get ep -n ingress-nginx
		NAME                                 ENDPOINTS                        AGE
		ingress-nginx                        10.244.3.17:443,10.244.3.17:80   17h
		ingress-nginx-controller             10.244.3.17:443,10.244.3.17:80   17h
		ingress-nginx-controller-admission   10.244.3.17:8443                 17h

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-yamls-practice (master)
		$ kc get po -n ingress-nginx
		NAME                                        READY   STATUS      RESTARTS   AGE
		ingress-nginx-admission-create-rpbc6        0/1     Completed   0          17h
		ingress-nginx-admission-patch-sktdh         0/1     Completed   0          17h
		ingress-nginx-controller-84cb46fccd-pw4fw   1/1     Running     1          17h

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-yamls-practice (master)
		$ kc get po -n ingress-nginx -o wide
		NAME                                        READY   STATUS      RESTARTS   AGE   IP            NODE                           NOMINATED NODE   READINESS GATES
		ingress-nginx-admission-create-rpbc6        0/1     Completed   0          17h   10.244.3.12   1be92a69641c.mylabserver.com   <none>           <none>
		ingress-nginx-admission-patch-sktdh         0/1     Completed   0          17h   10.244.1.10   ee87464c211c.mylabserver.com   <none>           <none>
		ingress-nginx-controller-84cb46fccd-pw4fw   1/1     Running     1          17h   10.244.3.17   1be92a69641c.mylabserver.com   <none>           <none>

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-yamls-practice (master)
		$ kc exec -it curlpod -c curl -- curl --head 10.97.190.168/kubeserve ***ingress-nginx svc
		HTTP/1.1 200 OK
		Server: nginx/1.19.2
		Date: Tue, 29 Sep 2020 17:56:36 GMT
		Connection: keep-alive


		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-yamls-practice (master)
		$ kc exec -it curlpod -c curl -- curl --head 10.110.196.105/kubeserve *** ingress-nginx-controller svc 
		HTTP/1.1 200 OK
		Server: nginx/1.19.2
		Date: Tue, 29 Sep 2020 17:56:52 GMT
		Connection: keep-alive

		-----

		after creating the ingress resource and creating the ingress-controller using the mandatory.yaml thingy, u can then use the pod and execute
		ingress connections based on the path mentioned in the ingress definition.

		$ kubectl get ingress -o yaml
		apiVersion: v1
		items:
		- apiVersion: extensions/v1beta1
		  kind: Ingress
		  metadata:
			annotations:
			  kubectl.kubernetes.io/last-applied-configuration: |
				{"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"name":"ingress-controller","namespace":"default"},"spec":{"rules":[{"http":{"paths":[{"backend":{"serviceName":"nginx-service","servicePort":80},"path":"/nginx"},{"backend":{"serviceName":"kubeserve-service","servicePort":80},"path":"/kubeserve"}]}}]}}
			creationTimestamp: "2019-12-24T01:30:57Z"
			generation: 1
			name: ingress-controller
			namespace: default
			resourceVersion: "51887"
			selfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/ingress-controller
			uid: 5d872f75-44ab-4bb1-9a2f-db022c149f32
		  spec:
			rules:
			- http:
				paths:
				- backend:
					serviceName: nginx-service
					servicePort: 80
				  path: /nginx
				- backend:
					serviceName: kubeserve-service
					servicePort: 80
				  path: /kubeserve
		  status:
			loadBalancer: {}
		kind: List
		metadata:
		  resourceVersion: ""
		  selfLink: ""
		cloud_user@chaitanyah3681c:~/prac$ kubectl get pods -n ingress-nginx
		NAME                                        READY   STATUS    RESTARTS   AGE
		nginx-ingress-controller-799dbf6fbd-56ctm   1/1     Running   0          18m
		cloud_user@chaitanyah3681c:~/prac$ kubectl get pods -n ingress-nginx -o wide
		NAME                                        READY   STATUS    RESTARTS   AGE   IP            NODE                              NOMINATED NODE   READINESS GATES
		nginx-ingress-controller-799dbf6fbd-56ctm   1/1     Running   0          18m   10.244.1.24   chaitanyah3682c.mylabserver.com   <none>           <none>
		cloud_user@chaitanyah3681c:~/prac$ curl http://10.244.1.24/kubeserve
		You've hit kubeserve-deployment-7f55cb56fd-54hzg
		cloud_user@chaitanyah3681c:~/prac$ curl http://10.244.1.24/nginx
		<html>
		<head><title>404 Not Found</title></head>
		<body>
		<center><h1>404 Not Found</h1></center>
		<hr><center>nginx/1.17.6</center>
		</body>
		</html>

		kc get ingress
		Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
		NAME       CLASS    HOSTS                     ADDRESS         PORTS   AGE
		ingress1   <none>   nginx.com,kubeserve.com   172.31.101.89   80      24m

		this ip not working. 

		i was having issues with 3 rpelicas of deployment for nginx:
			one of them was working fine while the other two were erroing out when get requests were made on 80
			so i added livenssProbe to this and this solved the issue as it only  kept good ones.
			        livenessProbe:
					  initialDelaySeconds: 10
					  httpGet:
						host: 127.0.0.1
						port: 80


		afterwards, the ip of the ingress-nginx service gets assigned to ingress as address in status spec. (describe ingress and ull see it under status/address)
		
		$ kc get ingress
		NAME              HOSTS   ADDRESS      PORTS   AGE
		default-ingress   *       10.32.0.27   80      3h35m
		cloud_user@chaitanyah3685c:~/kubernetes-code$

		also use a curl pod or something to run curl inside of it to access the ingress.
				  
		Edit the ingress rules:

		kubectl edit ingress

		View the existing ingress rules:

		kubectl describe ingress

		Curl the hostname of your Ingress resource:

		curl http://kubeserve2.example.com

		$ kc get svc -n ingress-nginx
		NAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
		ingress-nginx                        NodePort    10.97.190.168    <none>        80:32598/TCP,443:31735/TCP   7m19s
		ingress-nginx-controller             NodePort    10.110.196.105   <none>        80:31812/TCP,443:32201/TCP   20m
		ingress-nginx-controller-admission   ClusterIP   10.108.59.11     <none>        443/TCP                      20m

		$ kc describe ingress
		Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
		Name:             ingress1
		Namespace:        default
		Address:          172.31.101.89
		Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
		Rules:
		Host           Path  Backends
		----           ----  --------
		nginx.com
							nginx-svc:80   10.244.1.6:80,10.244.3.9:80)
		kubeserve.com
							kubeserve-svc:80   10.244.1.9:80,10.244.2.8:80)
		*
						/nginx   nginx-svc:80   10.244.1.6:80,10.244.3.9:80)
		*
						/kubeserve   kubeserve-svc:80   10.244.1.9:80,10.244.2.8:80)
		curlpod
							nginx-svc:80   10.244.1.6:80,10.244.3.9:80)
		Annotations:     <none>
		Events:
		Type    Reason  Age                 From                      Message
		----    ------  ----                ----                      -------
		Normal  CREATE  29m                 nginx-ingress-controller  Ingress default/ingress1
		Normal  UPDATE  116s (x3 over 29m)  nginx-ingress-controller  Ingress default/ingress1


		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-practice-09-23-20 (master)
		$ kc exec -it curlpod -c curl -- curl --head 10.97.190.168/kubeserve
		HTTP/1.1 200 OK
		Server: nginx/1.19.2
		Date: Tue, 29 Sep 2020 00:30:27 GMT
		Connection: keep-alive


		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-practice-09-23-20 (master)
		$ kc exec -it curlpod -c curl -- curl --head 10.97.190.168/nginx
		HTTP/1.1 404 Not Found
		Server: nginx/1.19.2
		Date: Tue, 29 Sep 2020 00:30:32 GMT
		Content-Type: text/html
		Content-Length: 153
		Connection: keep-alive

		$ kc exec -it curlpod -- curl --head 10.97.190.168
		Defaulting container name to curl.
		Use 'kubectl describe pod/curlpod -n default' to see all of the containers in this pod.
		HTTP/1.1 404 Not Found
		Server: nginx/1.19.2
		Date: Tue, 29 Sep 2020 00:38:03 GMT
		Content-Type: text/html
		Content-Length: 153
		Connection: keep-alive

	cluster dns::
		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-12-7 (master)
		$ kc exec -it curlpod -- nslookup kubernetes
		Server:    10.96.0.10
		Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

		Name:      kubernetes
		Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-12-7 (master)
		$ kc exec -it curlpod -- nslookup kubernetes.default.svc.cluster.local
		Server:    10.96.0.10
		Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

		Name:      kubernetes.default.svc.cluster.local
		Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-12-7 (master)
		$ kc exec -it curlpod -- nslookup nginx-service
		Server:    10.96.0.10
		Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

		Name:      nginx-service
		Address 1: 10.99.215.29 nginx-service.default.svc.cluster.local

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-12-7 (master)
		$ kc exec -it curlpod -- nslookup nginx-service.default.svc.cluster.local
		Server:    10.96.0.10
		Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

		Name:      nginx-service.default.svc.cluster.local
		Address 1: 10.99.215.29 nginx-service.default.svc.cluster.local

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-12-7 (master)
		$ kc get po -o wide
		NAME                                READY   STATUS    RESTARTS   AGE     IP              NODE                           NOMINATED NODE   READINESS GATES
		curlpod                             1/1     Running   13         3d21h   10.244.150.74   5575e104891c.mylabserver.com   <none>           <none>
		nginx-deployment-579755fcbb-8t2ff   1/1     Running   2          3d21h   10.244.150.75   5575e104891c.mylabserver.com   <none>           <none>
		nginx-deployment-579755fcbb-wsskb   1/1     Running   2          3d21h   10.244.150.76   5575e104891c.mylabserver.com   <none>           <none>

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-12-7 (master)
		$ kc exec -it curlpod -- nslookup 10-244-150-76.default.pod.cluster.local
		Server:    10.96.0.10
		Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

		Name:      10-244-150-76.default.pod.cluster.local
		Address 1: 10.244.150.76 10-244-150-76.nginx-service.default.svc.cluster.local

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/kube-12-7 (master)
		$ kc exec -it curlpod -- nslookup 10-244-150-76
		Server:    10.96.0.10
		Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

		nslookup: can't resolve '10-244-150-76'
		command terminated with exit code 1

		View the CoreDNS pods in the kube-system namespace:

		kubectl get pods -n kube-system

		View the CoreDNS deployment in your Kubernetes cluster:

		kubectl get deployments -n kube-system

		View the service that performs load balancing for the DNS server:

		kubectl get services -n kube-system

		Spec for the busybox pod:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: busybox
		  namespace: default
		spec:
		  containers:
		  - image: busybox:1.28.4
			command:
			  - sleep
			  - "3600"
			imagePullPolicy: IfNotPresent
			name: busybox
		  restartPolicy: Always

		View the resolv.conf file that contains the nameserver and search in DNS:

		kubectl exec -it busybox -- cat /etc/resolv.conf

		$ kc exec -it busybox -- cat /etc/resolv.conf
		search default.svc.cluster.local svc.cluster.local cluster.local us-west-2.compute.internal
		nameserver 10.32.0.10
		options ndots:5

		**has default svc and svc as options for domain matching; not pods.

		Look up the DNS name for the native Kubernetes service:

		kubectl exec -it busybox -- nslookup kubernetes

		Look up the DNS names of your pods:

		kubectl exec -ti busybox -- nslookup [pod-ip-address].default.pod.cluster.local (pod ip address in dashes.)

		Look up a service in your Kubernetes cluster:

		kubectl exec -it busybox -- nslookup kube-dns.kube-system.svc.cluster.local

		Get the logs of your CoreDNS pods:

		kubectl logs [coredns-pod-name]

		YAML spec for a headless service:

		apiVersion: v1
		kind: Service
		metadata:
		  name: kube-headless
		spec:
		  clusterIP: None
		  ports:
		  - port: 80
			targetPort: 8080
		  selector:
			app: kubserve2

			$ kubectl explain service.spec.clusterIP
			KIND:     Service
			VERSION:  v1

			FIELD:    clusterIP <string>

			DESCRIPTION:
				 clusterIP is the IP address of the service and is usually assigned randomly
				 by the master. If an address is specified manually and is not in use by
				 others, it will be allocated to the service; otherwise, creation of the
				 service will fail. This field can not be changed through updates. Valid
				 values are "None", empty string (""), or a valid IP address. "None" can be
				 specified for headless services when proxying is not required. Only applies
				 to types ClusterIP, NodePort, and LoadBalancer. Ignored if type is
				 ExternalName. More info:
				 https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies


		**headless service for when u dont need a clusterIP for that service. here, u can put that in
		an ingress resource so the clusterip is not needed. 
		this basically routes not to the cluster ip but to individual pods behind the service.

		nginx-svc is on default ns for nginx-deployment; 2 pods, one in default, the other in chai ns reaching out is shown below:

		$ kc exec -it curlpod -c curl -- curl --head nginx-svc
		HTTP/1.1 200 OK
		Server: nginx/1.19.3
		Date: Wed, 07 Oct 2020 23:40:44 GMT
		Content-Type: text/html
		Content-Length: 612
		Last-Modified: Tue, 29 Sep 2020 14:12:31 GMT
		Connection: keep-alive
		ETag: "5f7340cf-264"
		Accept-Ranges: bytes

		$ kc exec -it curlpod -n chai -c curl -- curl --head nginx-svc.default.svc.cluster.local
		HTTP/1.1 200 OK
		Server: nginx/1.19.3
		Date: Wed, 07 Oct 2020 23:38:35 GMT
		Content-Type: text/html
		Content-Length: 612
		Last-Modified: Tue, 29 Sep 2020 14:12:31 GMT
		Connection: keep-alive
		ETag: "5f7340cf-264"
		Accept-Ranges: bytes

		as the pod in chai ns has searches in resolv.conf as chai.svc then at svc level, here we gave full fqdn of the svc to reach nginx-svc on default ns.

		$ cat /etc/resolv.conf
		nameserver 10.96.0.10
		search chai.svc.cluster.local svc.cluster.local cluster.local us-west-2.compute.internal
		options ndots:5

		this works too:
		$ kc exec -it curlpod -n chai -c curl -- curl --head nginx-svc.default
		HTTP/1.1 200 OK
		Server: nginx/1.19.3
		Date: Wed, 07 Oct 2020 23:43:00 GMT
		Content-Type: text/html
		Content-Length: 612
		Last-Modified: Tue, 29 Sep 2020 14:12:31 GMT
		Connection: keep-alive
		ETag: "5f7340cf-264"
		Accept-Ranges: bytes




		YAML spec for a custom DNS pod:

		apiVersion: v1
		kind: Pod
		metadata:
		  namespace: default
		  name: dns-example
		spec:
		  containers:
			- name: test
			  image: nginx
		  dnsPolicy: "None"
		  dnsConfig:
			nameservers:
			  - 8.8.8.8
			searches:
			  - ns1.svc.cluster.local
			  - my.dns.search.suffix
			options:
			  - name: ndots
				value: "2"
			  - name: edns0
			  
		i setup a deployment/service in ns1 and another in ns2 and an ingress in ns1 (ingress is a namespaced resource);
		noticed only nginx endpoint was established using the ingress and was serviced by the ingress controller pod.
		not the other service in ns2.
		
		$ kubectl describe ingress -n ns1
		Name:             ingress2
		Namespace:        ns1
		Address:
		Default backend:  default-http-backend:80 (<none>)
		Rules:
		  Host  Path  Backends
		  ----  ----  --------
		  *
				/nginx       nginx-service:80 (10.244.1.20:80,10.244.1.21:80,10.244.2.23:80)
				/kubeserve   kubeserve-service:80 (<none>)
		Annotations:
		  kubectl.kubernetes.io/last-applied-configuration:  {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"name":"ingress2","namespace":"ns1"},"spec":{"rules":[{"http":{"paths":[{"backend":{"serviceName":"nginx-service","servicePort":80},"path":"/nginx"},{"backend":{"serviceName":"kubeserve-service","servicePort":80},"path":"/kubeserve"}]}}]}}

		Events:
		  Type    Reason  Age    From                      Message
		  ----    ------  ----   ----                      -------
		  Normal  CREATE  7m11s  nginx-ingress-controller  Ingress ns1/ingress2
		cloud_user@chaitanyah3681c:~$

		$ kubectl api-resources --namespaced=true | grep -i ingress
		ingresses                   ing          extensions                  true         Ingress
		ingresses                   ing          networking.k8s.io           true         Ingress
		cloud_user@chaitanyah3681c:~$


 scheduler:			  

	kubernetes scheduler:
	
	node affinity information:
	RESOURCE: nodeAffinity <Object>

	DESCRIPTION:
		 Describes node affinity scheduling rules for the pod.

		 Node affinity is a group of node affinity scheduling rules.

	FIELDS:
	   preferredDuringSchedulingIgnoredDuringExecution      <[]Object>
		 The scheduler will prefer to schedule pods to nodes that satisfy the
		 affinity expressions specified by this field, but it may choose a node that
		 violates one or more of the expressions. The node that is most preferred is
		 the one with the greatest sum of weights, i.e. for each node that meets all
		 of the scheduling requirements (resource request, requiredDuringScheduling
		 affinity expressions, etc.), compute a sum by iterating through the
		 elements of this field and adding "weight" to the sum if the node matches
		 the corresponding matchExpressions; the node(s) with the highest sum are
		 the most preferred.

	   requiredDuringSchedulingIgnoredDuringExecution       <Object>
		 If the affinity requirements specified by this field are not met at
		 scheduling time, the pod will not be scheduled onto the node. If the
		 affinity requirements specified by this field cease to be met at some point
		 during pod execution (e.g. due to an update), the system may or may not try
		 to eventually evict the pod from its node.

	(END)


		Label your node as being located in availability zone 1:

		kubectl label node chadcrowell1c.mylabserver.com availability-zone=zone1

		Label your node as dedicated infrastructure:

		kubectl label node chadcrowell1c.mylabserver.com share-type=dedicated

		Here is the YAML for the deployment to include the node affinity rules:

		apiVersion: extensions/v1beta1
		kind: Deployment
		metadata:
		  name: pref
		spec:
		  replicas: 5
		  template:
			metadata:
			  labels:
				app: pref
			spec:
			  affinity:
				nodeAffinity:
				  preferredDuringSchedulingIgnoredDuringExecution:
				  - weight: 80
					preference:
					  matchExpressions:
					  - key: availability-zone
						operator: In
						values:
						- zone1
				  - weight: 20
					preference:
					  matchExpressions:
					  - key: share-type
						operator: In
						values:
						- dedicated
			  containers:
			  - args:
				- sleep
				- "99999"
				image: busybox
				name: main

		**$ cat podScheduling.yaml
			apiVersion: v1
			kind: Pod
			metadata:
			  name: pod-affinity1
			spec:
			  containers:
			  - name: cont1
				image: busybox:1.28
				args:
				- sleep
				- "36000"
			  affinity:
				nodeAffinity:
				  preferredDuringSchedulingIgnoredDuringExecution:
				  - preference:
					  matchExpressions:
					  - key: node
						operator: In
						values:
						- worker1
					weight: 50


				with this code, when i ran this it chose worker1, but when i cordoned that node it chose worker2
				
				
				another example with requiredScheduling:
								

				apiVersion: v1
				kind: Pod
				metadata:
				  name: pod-affinity2
				spec:
				  containers:
				  - name: cont1
					image: busybox:1.28
					args:
					- sleep
					- "36000"
				  affinity:
					nodeAffinity:
					  requiredDuringSchedulingIgnoredDuringExecution:
						nodeSelectorTerms:
						- matchExpressions:
						  - key: node
							operator: In
							values:
							- worker2

				
		Create the deployment:

		kubectl create -f pref-deployment.yaml

		View the deployment:

		kubectl get deployments

		View which pods landed on which nodes:

		kubectl get pods -o wide


	multiple schedulers:


		**lifecycle parameter is used to run something, a script or call an api after starting or before ending. 
		$ kc explain deploy.spec.template.spec.containers.lifecycle
		KIND:     Deployment
		VERSION:  extensions/v1beta1

		RESOURCE: lifecycle <Object>

		DESCRIPTION:
			 Actions that the management system should take in response to container
			 lifecycle events. Cannot be updated.

			 Lifecycle describes actions that the management system should take in
			 response to container lifecycle events. For the PostStart and PreStop
			 lifecycle handlers, management of the container blocks until the action is
			 complete, unless the container process fails, in which case the handler is
			 aborted.

		FIELDS:
		   postStart    <Object>
			 PostStart is called immediately after a container is created. If the
			 handler fails, the container is terminated and restarted according to its
			 restart policy. Other management of the container blocks until the hook
			 completes. More info:
			 https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks

		   preStop      <Object>
			 PreStop is called immediately before a container is terminated due to an
			 API request or management event such as liveness probe failure, preemption,
			 resource contention, etc. The handler is not called if the container
			 crashes or exits. The reason for termination is passed to the handler. The
			 Pod's termination grace period countdown begins before the PreStop hooked
			 is executed. Regardless of the outcome of the handler, the container will
			 eventually terminate within the Pod's termination grace period. Other
			 management of the container blocks until the hook completes or until the
			 termination grace period is reached. More info:
			 https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks


---
		Periodic probe of container service readiness. Container will be removed
     	from service endpoints if the probe fails. Cannot be updated. More info:


		apiVersion: extensions/v1beta1
		kind: Deployment
		metadata:
		  labels:
			component: scheduler
			tier: control-plane
		  name: my-scheduler
		  namespace: kube-system
		spec:
		  replicas: 1
		  template:
			metadata:
			  labels: 
				component: scheduler
				tier: control-plane
				version: second
			spec:
			  containers:
			  - command: [/usr/local/bin/kube-scheduler, --address=0.0.0.0,
						  --scheduler-name=my-scheduler, --leader-elect=false]
				image: linuxacademycontent/content-kube-scheduler
				livenessProbe:
				  httpGet:
					path: /healthz
					port: 10251
				  initialDelaySeconds: 15
				name: kube-second-scheduler
				readinessProbe:
				  httpGet:
					path: /healthz
					port: 10251
				resources:
				  requests:
					cpu: '0.1'
				securityContext:
				  privileged: false
				volumeMounts: []
			  hostNetwork: false
			  hostPID: false
			  volumes: []

		Run the deployment for my-scheduler:

		kubectl create -f my-scheduler.yaml

		View your new scheduler in the kube-system namespace:

		kubectl get pods -n kube-system

		The pod YAML for pod 1:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: no-annotation
		  labels:
			name: multischeduler-example
		spec:
		  containers:
		  - name: pod-with-no-annotation-container
			image: k8s.gcr.io/pause:2.0

		The pod YAML for pod 2:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: annotation-default-scheduler
		  labels:
			name: multischeduler-example
		spec:
		  schedulerName: default-scheduler
		  containers:
		  - name: pod-with-default-annotation-container
			image: k8s.gcr.io/pause:2.0

		The pod YAML for pod 3:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: annotation-second-scheduler
		  labels:
			name: multischeduler-example
		spec:
		  schedulerName: my-scheduler
		  containers:
		  - name: pod-with-second-annotation-container
			image: k8s.gcr.io/pause:2.0

		View the pods as they are created:

		kubectl get pods -o wide
		
		----------
		In Kubernetes, you can run multiple schedulers simultaneously. You can then use different schedulers to schedule different pods. 
		You may, for example, want to set different rules for the scheduler to run all of your pods on one node. 
		In this lesson, I will show you how to deploy a new scheduler alongside your default scheduler and
		then schedule three different pods using the two schedulers.

		ClusterRole.yaml

		apiVersion: rbac.authorization.k8s.io/v1beta1
		kind: ClusterRole
		metadata:
		  name: csinodes-admin
		rules:
		- apiGroups: ["storage.k8s.io"]
		  resources: ["csinodes"]
		  verbs: ["get", "watch", "list"]

		ClusterRoleBinding.yaml

		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRoleBinding
		metadata:
		  name: read-csinodes-global
		subjects:
		- kind: ServiceAccount
		  name: my-scheduler
		  namespace: kube-system
		roleRef:
		  kind: ClusterRole
		  name: csinodes-admin
		  apiGroup: rbac.authorization.k8s.io

		Role.yaml

		apiVersion: rbac.authorization.k8s.io/v1
		kind: Role
		metadata:
		  name: system:serviceaccount:kube-system:my-scheduler
		  namespace: kube-system
		rules:
		- apiGroups:
		  - storage.k8s.io
		  resources:
		  - csinodes
		  verbs:
		  - get
		  - list
		  - watch

		RoleBinding.yaml

		apiVersion: rbac.authorization.k8s.io/v1
		kind: RoleBinding
		metadata:
		  name: read-csinodes
		  namespace: kube-system
		subjects:
		- kind: User
		  name: kubernetes-admin
		  apiGroup: rbac.authorization.k8s.io
		roleRef:
		  kind: Role 
		  name: system:serviceaccount:kube-system:my-scheduler
		  apiGroup: rbac.authorization.k8s.io

		Edit the existing kube-scheduler cluster role with kubectl edit clusterrole system:kube-scheduler and add the following:

		- apiGroups:
		  - ""
		  resourceNames:
		  - kube-scheduler
		  - my-scheduler
		  resources:
		  - endpoints
		  verbs:
		  - delete
		  - get
		  - patch
		  - update
		- apiGroups:
		  - storage.k8s.io
		  resources:
		  - storageclasses
		  verbs:
		  - watch
		  - list
		  - get

		My-scheduler.yaml

		apiVersion: v1
		kind: ServiceAccount
		metadata:
		  name: my-scheduler
		  namespace: kube-system
		---
		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRoleBinding
		metadata:
		  name: my-scheduler-as-kube-scheduler
		subjects:
		- kind: ServiceAccount
		  name: my-scheduler
		  namespace: kube-system
		roleRef:
		  kind: ClusterRole
		  name: system:kube-scheduler
		  apiGroup: rbac.authorization.k8s.io
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  labels:
			component: scheduler
			tier: control-plane
		  name: my-scheduler
		  namespace: kube-system
		spec:
		  selector:
			matchLabels:
			  component: scheduler
			  tier: control-plane
		  replicas: 1
		  template:
			metadata:
			  labels:
				component: scheduler
				tier: control-plane
				version: second
			spec:
			  serviceAccountName: my-scheduler
			  containers:
			  - command:
				- /usr/local/bin/kube-scheduler
				- --address=0.0.0.0
				- --leader-elect=false
				- --scheduler-name=my-scheduler
				image: chadmcrowell/custom-scheduler
				livenessProbe:
				  httpGet:
					path: /healthz
					port: 10251
				  initialDelaySeconds: 15
				name: kube-second-scheduler
				readinessProbe:
				  httpGet:
					path: /healthz
					port: 10251
				resources:
				  requests:
					cpu: '0.1'
				securityContext:
				  privileged: false
				volumeMounts: []
			  hostNetwork: false
			  hostPID: false
			  volumes: []

		Run the deployment for my-scheduler:

		kubectl create -f my-scheduler.yaml

		View your new scheduler in the kube-system namespace:

		kubectl get pods -n kube-system

		pod1.yaml

		apiVersion: v1
		kind: Pod
		metadata:
		  name: no-annotation
		  labels:
			name: multischeduler-example
		spec:
		  containers:
		  - name: pod-with-no-annotation-container
			image: k8s.gcr.io/pause:2.0

		pod2.yaml

		apiVersion: v1
		kind: Pod
		metadata:
		  name: annotation-default-scheduler
		  labels:
			name: multischeduler-example
		spec:
		  schedulerName: default-scheduler
		  containers:
		  - name: pod-with-default-annotation-container
			image: k8s.gcr.io/pause:2.0

		pod3.yaml

		apiVersion: v1
		kind: Pod
		metadata:
		  name: annotation-second-scheduler
		  labels:
			name: multischeduler-example
		spec:
		  schedulerName: my-scheduler
		  containers:
		  - name: pod-with-second-annotation-container
			image: k8s.gcr.io/pause:2.0

		View the pods as they are created:

		kubectl get pods -o wide


	resource limits and labels::

		View the capacity and the allocatable info from a node:

		kubectl describe nodes

		The pod YAML for a pod with requests:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: resource-pod1
		spec:
		  nodeSelector:
			kubernetes.io/hostname: "chadcrowell3c.mylabserver.com"
		  containers:
		  - image: busybox
			command: ["dd", "if=/dev/zero", "of=/dev/null"]
			name: pod1
			resources:
			  requests:
				cpu: 800m
				memory: 20Mi

		Create the requests pod:

		kubectl create -f resource-pod1.yaml

		View the pods and nodes they landed on:

		kubectl get pods -o wide

		The YAML for a pod that has a large request:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: resource-pod2
		spec:
		  nodeSelector:
			kubernetes.io/hostname: "chadcrowell3c.mylabserver.com"
		  containers:
		  - image: busybox
			command: ["dd", "if=/dev/zero", "of=/dev/null"]
			name: pod2
			resources:
			  requests:
				cpu: 1000m
				memory: 20Mi

		Create the pod with 1000 millicore request:

		kubectl create -f resource-pod2.yaml

		See why the pod with a large request didnt get scheduled:

		kubectl describe resource-pod2

		Look at the total requests per node:

		kubectl describe nodes chadcrowell3c.mylabserver.com

		Delete the first pod to make room for the pod with a large request:

		kubectl delete pods resource-pod1

		Watch as the first pod is terminated and the second pod is started:

		kubectl get pods -o wide -w

		The YAML for a pod that has limits:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: limited-pod
		spec:
		  containers:
		  - image: busybox
			command: ["dd", "if=/dev/zero", "of=/dev/null"]
			name: main
			resources:
			  limits:
				cpu: 1
				memory: 20Mi

		Create a pod with limits:

		kubectl create -f limited-pod.yaml

		Use the exec utility to use the top command:

		kubectl exec -it limited-pod top


	daemon sets and manually schedule pods:

		Find the DaemonSet pods that exist in your kubeadm cluster:

		kubectl get pods -n kube-system -o wide
		
		daemonset refers to the worker nodes in the cluster / pods in each of the worker node on the cluster.

		Delete a DaemonSet pod and see what happens:
		
		kubectl delete pods [pod_name] -n kube-system

		Give the node a label to signify it has SSD:

		kubectl label node[node_name] disk=ssd

		The YAML for a DaemonSet:

		Running Pods on Only Some Nodes
		If you specify a .spec.template.spec.nodeSelector, then the DaemonSet controller will create Pods on nodes which match that node selector.
		Likewise if you specify a .spec.template.spec.affinity,
		then DaemonSet controller will create Pods on nodes which match that node affinity. 
		If you do not specify either, then the DaemonSet controller will create Pods on all nodes


		kubectl label node `hostname -f | sed 's/1/2/'` disk=ssd --overwrite=true

		apiVersion: apps/v1beta2
		kind: DaemonSet
		metadata:
		  name: ssd-monitor
		spec:
		  selector:
			matchLabels:
			  app: ssd-monitor
		  template:
			metadata:
			  labels:
				app: ssd-monitor
			spec:
			  nodeSelector:
				disk: ssd
			  containers:
			  - name: main
				image: linuxacademycontent/ssd-monitor

		Create a DaemonSet from a YAML spec:

		kubectl create -f ssd-monitor.yaml

		Label another node to specify it has SSD:

		kubectl label node chadcrowell2c.mylabserver.com disk=ssd

		View the DaemonSet pods that have been deployed:

		kubectl get pods -o wide

		Remove the label from a node and watch the DaemonSet pod terminate:

		kubectl label node chadcrowell3c.mylabserver.com disk-

		Change the label on a node to change it to spinning disk:

		kubectl label node chadcrowell2c.mylabserver.com disk=hdd --overwrite

		Pick the label to choose for your DaemonSet:

		kubectl get nodes chadcrowell3c.mylabserver.com --show-labels

		**nodeSelector label ensures that only node with the requisite label gets the pod even if they are daemon set pods.
		before adding the label, no nodes had any pods; after adding the node, the node had the one pod.
		

	<<NOW HERE>>displaying scheduler events:

		View the name of the scheduler pod:

		kubectl get pods -n kube-system

		Get the information about your scheduler pod events:

		kubectl describe pods [scheduler_pod_name] -n kube-system

		View the events in your default namespace:

		kubectl get events

		View the events in your kube-system namespace:

		kubectl get events -n kube-system

		Delete all the pods in your default namespace:

		kubectl delete pods --all

		Watch events as they are appearing in real time:

		kubectl get events -w

		View the logs from the scheduler pod:

		kubectl logs [kube_scheduler_pod_name] -n kube-system

		The location of a systemd service scheduler pod:

		/var/log/kube-scheduler.log



app lifecycle mgmt:

	deployments, updates, rollouts  and rollbacks:

		The YAML for a deployment:

		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: kubeserve
		spec:
		  replicas: 3
		  selector:
			matchLabels:
			  app: kubeserve
		  template:
			metadata:
			  name: kubeserve
			  labels:
				app: kubeserve
			spec:
			  containers:
			  - image: linuxacademycontent/kubeserve:v1
				name: app

		Create a deployment with a record (for rollbacks):

		kubectl create -f kubeserve-deployment.yaml --record

		Check the status of the rollout:

		kubectl rollout status deployments kubeserve

		View the ReplicaSets in your cluster:

		kubectl get replicasets

		Scale up your deployment by adding more replicas:

		kubectl scale deployment kubeserve --replicas=5

		Expose the deployment and provide it a service:

		kubectl expose deployment kubeserve --port 80 --target-port 80 --type NodePort

		Set the minReadySeconds attribute to your deployment:

		kubectl patch deployment kubeserve -p '{"spec": {"minReadySeconds": 10}}'

		Use kubectl apply to update a deployment:

		kubectl apply -f kubeserve-deployment.yaml

		Use kubectl replace to replace an existing deployment:

		kubectl replace -f kubeserve-deployment.yaml

		Run this curl look while the update happens:

		while true; do curl http://10.105.31.119; done

		Perform the rolling update:

		$ kc set image deploy/kubeserve-deploy cont1=linuxacademycontent/kubeserve:v2 --v 6
		I0218 18:39:09.846762   12140 loader.go:375] Config loaded from file:  C:\Users\Chaitanya.Kolluru/.kube/config
		I0218 18:39:10.106004   12140 round_trippers.go:443] GET https://b8513c11f91c.mylabserver.com:6443/apis/apps/v1/namespaces/default/deployments/kubeserve-deploy 200 OK in 251 milliseconds
		I0218 18:39:10.192184   12140 round_trippers.go:443] PATCH https://b8513c11f91c.mylabserver.com:6443/apis/apps/v1/namespaces/default/deployments/kubeserve-deploy?fieldManager=kubectl-set 200 OK in 70 milliseconds
		deployment.apps/kubeserve-deploy image updated

		**** --v 6 ensures its set to verbose debug mode.

		$ kc rollout history deploy/kubeserve-deploy
		deployment.apps/kubeserve-deploy
		REVISION  CHANGE-CAUSE
		1         <none>
		2         kubectl.exe set image deploy/kubeserve-deploy cont1=linuxacademycontent/kubeserve:v2 --v=6 --record=true

		Describe a certain ReplicaSet:

		kubectl describe replicasets kubeserve-[hash]

		Apply the rolling update to version 3 (buggy):

		kubectl set image deployment kubeserve app=linuxacademycontent/kubeserve:v3

		Undo the rollout and roll back to the previous version:

		$ kc rollout undo deploy/kubeserve-deploy
		deployment.apps/kubeserve-deploy rolled back
		
		after u undo a rollout; in this case 3 was rolled back to 2; it shows the history as 1,3 and 4 suggesting that 2 is the current version renumbered as 4.
		$ kc rollout history deployment/lifecycle
		deployment.apps/lifecycle 
		REVISION  CHANGE-CAUSE
		1         kubectl apply --filename=lifecycle-mgmt.yaml --record=true
		3         kubectl apply --filename=lifecycle-mgmt.yaml --record=true
		4         kubectl apply --filename=lifecycle-mgmt.yaml --record=true


		Look at the rollout history:

		kubectl rollout history deployment kubeserve

		Roll back to a certain revision:

		kubectl rollout undo deployment kubeserve --to-revision=2

		Pause the rollout in the middle of a rolling update (canary release):

		kubectl rollout pause deployment kubeserve

		Resume the rollout after the rolling update looks good:

		kubectl rollout resume deployment kubeserve


	for high availability and scale:
			
		The YAML for a readiness probe:

		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: kubeserve
		spec:
		  replicas: 3
		  selector:
			matchLabels:
			  app: kubeserve
		  minReadySeconds: 10
		  strategy:
			rollingUpdate:
			  maxSurge: 1
			  maxUnavailable: 0
			type: RollingUpdate
		  template:
			metadata:
			  name: kubeserve
			  labels:
				app: kubeserve
			spec:
			  containers:
			  - image: linuxacademycontent/kubeserve:v3
				name: app
				readinessProbe:
				  periodSeconds: 1
				  httpGet:
					path: /
					port: 80

		Apply the readiness probe:

		kubectl apply -f kubeserve-deployment-readiness.yaml

		View the rollout status:

		kubectl rollout status deployment kubeserve

		Describe deployment:

		kubectl describe deployment

		Create a ConfigMap with two keys:

		kubectl create configmap appconfig --from-literal=key1=value1 --from-literal=key2=value2

		Get the YAML back out from the ConfigMap:

		kubectl get configmap appconfig -o yaml

		The YAML for the ConfigMap pod:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: configmap-pod
		spec:
		  containers:
		  - name: app-container
			image: busybox:1.28
			command: ['sh', '-c', "echo $(MY_VAR) && sleep 3600"]
			env:
			- name: MY_VAR
			  valueFrom:
				configMapKeyRef:
				  name: appconfig
				  key: key1 

		Create the pod that is passing the ConfigMap data:

		kubectl apply -f configmap-pod.yaml

		Get the logs from the pod displaying the value:

		kubectl logs configmap-pod

		The YAML for a pod that has a ConfigMap volume attached:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: configmap-volume-pod
		spec:
		  containers:
		  - name: app-container
			image: busybox
			command: ['sh', '-c', "echo $(MY_VAR) && sleep 3600"]
			volumeMounts:
			  - name: configmapvolume
				mountPath: /etc/config
		  volumes:
			- name: configmapvolume
			  configMap:
				name: appconfig

		**config map and secrets using env and as a volume mounted on the pod:
		$ cat pod-using-cm-sec.yaml
		apiVersion: v1
		kind: ConfigMap
		metadata:
		  name: cm-literal
		data:
		  key1: value1
		  key2: value2


		---

		apiVersion: v1
		kind: Secret
		metadata:
		  name: sec-literal
		stringData:
		  cert: value
		  key: value

		---


		apiVersion: v1
		kind: Pod
		metadata:
		  name: pod1
		spec:
		  containers:
		  - name: cont1
			image: busybox:1.28
			args:
			- sleep
			- "10000000"
			volumeMounts:
			- name: vol-cm
			  mountPath: /etc/vol-cm
			- name: vol-sec
			  mountPath: /etc/vol-sec
			env:
			- name: KEY1
			  valueFrom:
				configMapKeyRef:
				  name: cm-literal
				  key: key1
			- name: CERT
			  valueFrom:
				secretKeyRef:
				  name: sec-literal
				  key: cert
		  volumes:
		  - name: vol-cm
			configMap:
			  name: cm-literal
		  - name: vol-sec
			secret:
			  secretName: sec-literal




		Create the ConfigMap volume pod:

		kubectl apply -f configmap-volume-pod.yaml

		Get the keys from the volume on the container:

		kubectl exec configmap-volume-pod -- ls /etc/config

		Get the values from the volume on the pod:

		kubectl exec configmap-volume-pod -- cat /etc/config/key1

		The YAML for a secret:

		apiVersion: v1
		kind: Secret
		metadata:
		  name: appsecret
		stringData:
		  cert: value
		  key: value

		Create the secret:

		kubectl apply -f appsecret.yaml

		The YAML for a pod that will use the secret:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: secret-pod
		spec:
		  containers:
		  - name: app-container
			image: busybox
			command: ['sh', '-c', "echo Hello, Kubernetes! && sleep 3600"]
			env:
			- name: MY_CERT
			  valueFrom:
				secretKeyRef:
				  name: appsecret
				  key: cert

		Create the pod that has attached secret data:

		kubectl apply -f secret-pod.yaml

		Open a shell and echo the environment variable:

		kubectl exec -it secret-pod -- sh

		echo $MY_CERT

		The YAML for a pod that will access the secret from a volume:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: secret-volume-pod
		spec:
		  containers:
		  - name: app-container
			image: busybox
			command: ['sh', '-c', "echo $(MY_VAR) && sleep 3600"]
			volumeMounts:
			  - name: secretvolume
				mountPath: /etc/certs
		  volumes:
			- name: secretvolume
			  secret:
				secretName: appsecret

		Create the pod with volume attached with secrets:

		kubectl apply -f secret-volume-pod.yaml

		Get the keys from the volume mounted to the container with the secrets:

		kubectl exec secret-volume-pod -- ls /etc/certs



	self healing app:

		The YAML for a ReplicaSet:

		apiVersion: apps/v1
		kind: ReplicaSet
		metadata:
		  name: myreplicaset
		  labels:
			app: app
			tier: frontend
		spec:
		  replicas: 3
		  selector:
			matchLabels:
			  tier: frontend
		  template:
			metadata:
			  labels:
				tier: frontend
			spec:
			  containers:
			  - name: main
				image: linuxacademycontent/kubeserve

		Create the ReplicaSet:

		kubectl apply -f replicaset.yaml

		The YAML for a pod with the same label as a ReplicaSet:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: pod1
		  labels:
			tier: frontend
		spec:
		  containers:
		  - name: main
			image: linuxacademycontent/kubeserve

		Create the pod with the same label:

		kubectl apply -f pod-replica.yaml

		Watch the pod get terminated:

		kubectl get pods -w

		** i invoked the 3 replicas rs and pod with the same alabel at the same time and i notice
		only 2 replicas of the rs and the pod were created, to match the replica count of 3.
		on deleting the pod i can see the rs now having 3 replicas.
		
		$ kubectl get pods
		NAME        READY   STATUS        RESTARTS   AGE
		pod1        0/1     Terminating   0          3s
		rs1-lhdgh   1/1     Running       0          2m18s
		rs1-ntg8f   1/1     Running       0          2m18s
		rs1-sq4pn   1/1     Running       0          91s
		cloud_user@chaitanyah3681c:~$

		**under volumeClaimTemplates, ud have to give metadata: -- name: as vol matching the volumeMounts in the container definition.
		stateful tries to append volumeClaimTempalte name to stateful set name with the -integer at the end.
				
		$ kc get statefulset
		NAME          READY   AGE
		stateful-pv   4/4     22s
		cloud_user@chaitanyah3685c:~/kubernetes-code$ kc get pods
		NAME                                    READY   STATUS    RESTARTS   AGE
		busybox                                 1/1     Running   3          2d4h
		curlpod                                 1/1     Running   5          5d
		kubeserve-deployment-5cd567d88c-6q79d   1/1     Running   4          2d9h
		kubeserve-deployment-5cd567d88c-j87ns   1/1     Running   4          2d9h
		kubeserve-deployment-5cd567d88c-n8jdv   1/1     Running   4          2d9h
		nginx-deployment-6d8b997474-v4w6n       1/1     Running   3          2d5h
		stateful-pv-0                           1/1     Running   0          24s
		stateful-pv-1                           1/1     Running   0          21s
		stateful-pv-2                           1/1     Running   0          18s
		stateful-pv-3                           1/1     Running   0          16s
		cloud_user@chaitanyah3685c:~/kubernetes-code$ kc get pv
		NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS    REASON   AGE
		pv1    100Mi      RWO            Retain           Bound    default/vol-stateful-pv-1   local-storage            28s
		pv2    100Mi      RWO            Retain           Bound    default/vol-stateful-pv-0   local-storage            28s
		pv3    100Mi      RWO            Retain           Bound    default/vol-stateful-pv-2   local-storage            28s
		pv4    100Mi      RWO            Retain           Bound    default/vol-stateful-pv-3   local-storage            28s
		cloud_user@chaitanyah3685c:~/kubernetes-code$ kc get pvc
		NAME                STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
		vol-stateful-pv-0   Bound    pv2      100Mi      RWO            local-storage   30s
		vol-stateful-pv-1   Bound    pv1      100Mi      RWO            local-storage   27s
		vol-stateful-pv-2   Bound    pv3      100Mi      RWO            local-storage   24s
		vol-stateful-pv-3   Bound    pv4      100Mi      RWO            local-storage   22s
		cloud_user@chaitanyah3685c:~/kubernetes-code$

		The YAML for a StatefulSet:

		apiVersion: apps/v1
		kind: StatefulSet
		metadata:
		  name: statefulset
		  namespace: practice3
		spec:
		  serviceName: "statefulset"
		  replicas: 2
		  selector:
			matchLabels:
			  app: statefulset
		  template:
			metadata:
			  labels:
				app: statefulset
			spec:
			  containers:
			  - name: kk
				image: nginx
				ports:
				- containerPort: 80
				volumeMounts:
				- name: vol1
				  mountPath: /usr/share/nginx/html
		  volumeClaimTemplates:
		  - metadata:
			  name: vol1
			spec:
			  accessModes: ["ReadWriteOnce"]
			  resources:
				requests:
				  storage: 1Gi

		** u can use a single hostPath to create multiple pvs and use it to multiple replicas of a statefulset.
		** volumeClaimTempaltes thingy. code pasted below.
		
		hostname of the pod after creation; is in the format like so:
		# hostname -f
		stateful-pv-0.stateful-pv.default.svc.cluster.local

		
		kind: PersistentVolume
		metadata:
		  name: pv1
		spec:
		  accessModes:
		  - ReadWriteOnce
		  storageClassName: local-storage
		  capacity:
			storage: 500Mi
		  hostPath:
			path: /opt/testing

		---


		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: pv2
		spec:
		  accessModes:
		  - ReadWriteOnce
		  storageClassName: local-storage
		  capacity:
			storage: 500Mi
		  hostPath:
			path: /opt/testing


		---


		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: pv3
		spec:
		  accessModes:
		  - ReadWriteOnce
		  storageClassName: local-storage
		  capacity:
			storage: 500Mi
		  hostPath:
			path: /opt/testing


		---


		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: pv4
		spec:
		  accessModes:
		  - ReadWriteOnce
		  storageClassName: local-storage
		  capacity:
			storage: 500Mi
		  hostPath:
			path: /opt/testing

		---


		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: pv5
		spec:
		  accessModes:
		  - ReadWriteOnce
		  storageClassName: local-storage
		  capacity:
			storage: 500Mi
		  hostPath:
			path: /opt/testing


	**volumeClaimTempaltes are a statefulset specialty; only present in that case. not in the case of deployments.

	**$ kubectl explain statefulset.spec.serviceName
	KIND:     StatefulSet
	VERSION:  apps/v1

	FIELD:    serviceName <string>

	DESCRIPTION:
		 serviceName is the name of the service that governs this StatefulSet. This
		 service must exist before the StatefulSet, and is responsible for the
		 network identity of the set. Pods get DNS/hostnames that follow the
		 pattern: pod-specific-string.serviceName.default.svc.cluster.local where
		 "pod-specific-string" is managed by the StatefulSet controller.
	cloud_user@chaitanyah3681c:~$

	**serviceName is needed and pods will have a different dns name as can be explained in the doc.

	$ cat statefulset.yaml
	apiVersion: apps/v1
	kind: StatefulSet
	metadata:
	  name: stateful
	  labels:
		app: stateful
	spec:
	  replicas: 5
	  serviceName: stateful
	  selector:
		matchLabels:
		  app: stateful
	  template:
		metadata:
		  name: stateful
		  labels:
			app: stateful
		spec:
		  containers:
		  - name: app1
			image: nginx
			ports:
			- containerPort: 80
			volumeMounts:
			- name: vol1
			  mountPath: /usr/share/nginx/html
	  volumeClaimTemplates:
	  - metadata:
		  name: vol1
		spec:
		  storageClassName: local-storage
		  accessModes:
		  - ReadWriteOnce
		  resources:
			requests:
			 storage: 500Mi


		Create the StatefulSet:

		kubectl apply -f statefulset.yaml

		View all StatefulSets in the cluster:

		kubectl get statefulsets

		Describe the StatefulSets:

		kubectl describe statefulsets

** had to create the hsotPath dir in all worker nodes to make sure the pv gets bound to pvc and to the pod in the statfulset.

storage: 

	persistent volumes:

		In the Google Cloud Engine, find the region your cluster is in:

		gcloud container clusters list

		Using Google Cloud, create a persistent disk in the same region as your cluster:

		gcloud compute disks create --size=1GiB --zone=us-central1-a mongodb

		The YAML for a pod that will use persistent disk:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: mongodb 
		spec:
		  volumes:
		  - name: mongodb-data
			gcePersistentDisk:
			  pdName: mongodb
			  fsType: ext4
		  containers:
		  - image: mongo
			name: mongodb
			volumeMounts:
			- name: mongodb-data
			  mountPath: /data/db
			ports:
			- containerPort: 27017
			  protocol: TCP

		Create the pod with disk attached and mounted:

		kubectl apply -f mongodb-pod.yaml

		See which node the pod landed on:

		kubectl get pods -o wide

		Connect to the mongodb shell:

		kubectl exec -it mongodb mongo

		Switch to the mystore database in the mongodb shell:

		use mystore

		Create a JSON document to insert into the database:

		db.foo.insert({name:'foo'})

		View the document you just created:

		db.foo.find()

		Exit from the mongodb shell:

		exit

		Delete the pod:

		kubectl delete pod mongodb

		Create a new pod with the same attached disk:

		kubectl apply -f mongodb-pod.yaml

		Check to see which node the pod landed on:

		kubectl get pods -o wide

		Drain the node (if the pod is on the same node as before):

		kubectl drain [node_name] --ignore-daemonsets

		Once the pod is on a different node, access the mongodb shell again:

		kubectl exec -it mongodb mongo

		Access the mystore database again:

		use mystore

		Find the document you created from before:

		db.foo.find()

		The YAML for a PersistentVolume object in Kubernetes:

		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: mongodb-pv
		spec:
		  capacity: 
			storage: 1Gi
		  accessModes:
			- ReadWriteOnce
			- ReadOnlyMany
		  persistentVolumeReclaimPolicy: Retain
		  gcePersistentDisk:
			pdName: mongodb
			fsType: ext4

		Create the Persistent Volume resource:

		kubectl apply -f mongodb-persistentvolume.yaml

		View our Persistent Volumes:

		kubectl get persistentvolumes


	access modes:
	
		tell kubernetes if it can be read from and written to by one or many nodes.
		- ReadWriteOnce - only 1 node can mount the volume for writing and reading
		- ReadOnlyMany - allows many nodes to access teh volume for reading
		- ReadWriteMany - allows multiple nodes to mount the volume for reading and writing
		
		imp points:
			- this means mounting by the noede and is at a node level
			- the volume can only be mounted using one access mode at a time, even though it supports more than once.
			  in the previous example, u cannot have it read and written by one node and have the content read by another
			  node at the same time.

	
	persistent volume claims:

		The YAML for a PVC:

		apiVersion: v1
		kind: PersistentVolumeClaim
		metadata:
		  name: mongodb-pvc 
		spec:
		  resources:
			requests:
			  storage: 1Gi
		  accessModes:
		  - ReadWriteOnce
		  storageClassName: ""

		Create a PVC:

		kubectl apply -f mongodb-pvc.yaml

		View the PVC in the cluster:

		kubectl get pvc

		View the PV to ensure its bound:

		kubectl get pv

		The YAML for a pod that uses a PVC:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: mongodb 
		spec:
		  containers:
		  - image: mongo
			name: mongodb
			volumeMounts:
			- name: mongodb-data
			  mountPath: /data/db
			ports:
			- containerPort: 27017
			  protocol: TCP
		  volumes:
		  - name: mongodb-data
			persistentVolumeClaim:
			  claimName: mongodb-pvc


		Create the pod with the attached storage:

		kubectl apply -f mongo-pvc-pod.yaml

		Access the mogodb shell:

		kubectl exec -it mongodb mongo

		Find the JSON document created in previous lessons:

		db.foo.find()

		Delete the mongodb pod:

		kubectl delete pod mogodb

		Delete the mongodb-pvc PVC:

		kubectl delete pvc mongodb-pvc

		Check the status of the PV:

		kubectl get pv

		The YAML for the PV to show its reclaim policy:

		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: mongodb-pv
		spec:
		  capacity: 
			storage: 1Gi
		  accessModes:
			- ReadWriteOnce
			- ReadOnlyMany
		  persistentVolumeReclaimPolicy: Retain  ** retain retains the pv after claim ends; Delete deletes the pv.
		  gcePersistentDisk:
			pdName: mongodb
			fsType: ext4

		***one thing with 1 pod with 1 pvc and 1 pv with local-storage storageClass (hostPath):
		I had one pvc with one pv (hostPath/local storage sc) and 2 busybox pods and i saw the volumes being unique.
		this is coz the pods were on diff nodes; makes sense u dont want to use hsotPath for pv except for testing.

		busybox1:
		apiVersion: v1
		kind: Pod
		metadata:
		name: busybox
		spec:
		containers:
		- args:
			- sleep
			- "1000"
			image: radial/busyboxplus:curl
			name: busybox
			volumeMounts:
			- name: vol1
			mountPath: /opt/test
		volumes:
		- name: vol1
			persistentVolumeClaim:
			claimName: pvc1

		---

		apiVersion: v1
		kind: PersistentVolumeClaim
		metadata:
		name: pvc1
		spec:
		accessModes:
		- ReadWriteMany
		resources:
			requests:
			storage: 500Mi
		storageClassName: local-storage

		busybox2:
		apiVersion: v1
		kind: Pod
		metadata:
		name: busybox2
		spec:
		containers:
		- args:
			- sleep
			- "1000"
			image: radial/busyboxplus:curl
			name: busybox
			volumeMounts:
			- name: vol1
			mountPath: /opt/test
		volumes:
		- name: vol1
			persistentVolumeClaim:
			claimName: pvc1

		pv:
		apiVersion: v1
		kind: PersistentVolume
		metadata:
		name: pv1
		spec:
		persistentVolumeReclaimPolicy: Delete
		storageClassName: local-storage
		hostPath:
			path: /opt/testing
		capacity:
			storage: 500Mi
		accessModes:
		- ReadWriteMany

		$ kc get po
		NAME       READY   STATUS    RESTARTS   AGE
		busybox    1/1     Running   0          11m
		busybox2   1/1     Running   0          8m29s

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/02-18-21 (master)
		$ kc get pvc
		NAME   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
		pvc1   Bound    pv1      500Mi      RWX            local-storage   11m

		Chaitanya.Kolluru@9XFNN53 MINGW64 ~/Desktop/AA-new-2/tuna1/kubernetes-helm-istio/kube-practice/02-18-21 (master)
		$ kc get pv
		NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM          STORAGECLASS    REASON   AGE
		pv1    500Mi      RWX            Delete           Bound       default/pvc1   local-storage            12m

		[ root@busybox:/ ]$ ls -ltr /opt/test
		total 0
		-rw-r--r--    1 root     root           0 Feb 19 04:52 file{1,2,3,4,5}

		[ root@busybox2:/ ]$ ls -ltr /opt/test
		total 0
		-rw-r--r--    1 root     root           0 Feb 19 04:57 ggg

		Now, I deleted busybox2 and recreated it with nodeName set to the same one as first one. Now fiel volume 
		one box2 shows whats on busybox1

		busybox2:
		apiVersion: v1
		kind: Pod
		metadata:
		name: busybox2
		spec:
		nodeName: e5b5ffe97b1c.mylabserver.com
		containers:
		- args:
			- sleep
			- "1000"
			image: radial/busyboxplus:curl
			name: busybox
			volumeMounts:
			- name: vol1
			mountPath: /opt/test
		volumes:
		- name: vol1
			persistentVolumeClaim:
			claimName: pvc1

		$ kc exec -it busybox2 -- sh
		sh: shopt: not found
		[ root@busybox2:/ ]$ ls -ltr /opt/test
		total 0
		-rw-r--r--    1 root     root           0 Feb 19 04:52 file{1,2,3,4,5}

	storage object in use protection:

		when a pv is being used by a pod, ull see under finalizers under pv as pv-protection; same also under describe thing for pvc.
		u cant delete the pv when a pod is using it by referencing the pvc.
	
		See the PV protection on your volume:

		kubectl describe pv mongodb-pv

		See the PVC protection for your claim:

		kubectl describe pvc mongodb-pvc

		Delete the PVC:

		kubectl delete pvc mongodb-pvc

		See that the PVC is terminated, but the volume is still attached to pod:

		kubectl get pvc

		Try to access the data, even though we just deleted the PVC:

		kubectl exec -it mongodb mongo
		use mystore
		db.foo.find()

		Delete the pod, which finally deletes the PVC:

		kubectl delete pods mongodb

		Show that the PVC is deleted:

		kubectl get pvc

		YAML for a StorageClass object:

		apiVersion: storage.k8s.io/v1
		kind: StorageClass
		metadata:
		  name: fast
		provisioner: kubernetes.io/gce-pd
		parameters:
		  type: pd-ssd

		Create the StorageClass type "fast":

		kubectl apply -f sc-fast.yaml

		Change the PVC to include the new StorageClass object:

		apiVersion: v1
		kind: PersistentVolumeClaim
		metadata:
		  name: mongodb-pvc 
		spec:
		  storageClassName: fast
		  resources:
			requests:
			  storage: 100Mi
		  accessModes:
			- ReadWriteOnce

		Create the PVC with automatically provisioned storage:

		kubectl apply -f mongodb-pvc.yaml

		View the PVC with new StorageClass:

		kubectl get pvc

		View the newly provisioned storage:

		kubectl get pv

		The YAML for a hostPath PV:

		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: pv-hostpath
		spec:
		  storageClassName: local-storage
		  capacity:
			storage: 1Gi
		  accessModes:
			- ReadWriteOnce
		  hostPath:
			path: "/mnt/data"

		The YAML for a pod with an empty directory volume:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: emptydir-pod
		spec:
		  containers:
		  - image: busybox
			name: busybox
			command: ["/bin/sh", "-c", "while true; do sleep 3600; done"]
			volumeMounts:
			- mountPath: /tmp/storage
			  name: vol
		  volumes:
		  - name: vol
			emptyDir: {}

	** emptyDir     <Object>
     EmptyDir represents a temporary directory that shares a pod's lifetime.
     More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir

		**pv status can be available, bound or released.
	
	
applications and storage:

The YAML for our StorageClass object:

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd

The YAML for our PVC:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kubeserve-pvc 
spec:
  storageClassName: fast
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce

Create our StorageClass object:

kubectl apply -f storageclass-fast.yaml

View the StorageClass objects in your cluster:

kubectl get sc

Create our PVC:

kubectl apply -f kubeserve-pvc.yaml

View the PVC created in our cluster:

kubectl get pvc

View our automatically provisioned PV:

kubectl get pv

The YAML for our deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      containers:
      - env:
        - name: app
          value: "1"
        image: linuxacademycontent/kubeserve:v1
        name: app
        volumeMounts:
        - mountPath: /data
          name: volume-data
      volumes:
      - name: volume-data
        persistentVolumeClaim:
          claimName: kubeserve-pvc

Create our deployment and attach the storage to the pods:

kubectl apply -f kubeserve-deployment.yaml

Check the status of the rollout:

kubectl rollout status deployments kubeserve

Check the pods have been created:

kubectl get pods

Connect to our pod and create a file on the PV:

kubectl exec -it [pod-name] -- touch /data/file1.txt

Connect to our pod and list the contents of the /data directory:

kubectl exec -it [pod-name] -- ls /data


*practice yaml that creates a pv and pvc off of local-storage and a deployment that uses that storage.

$ cat kubeserve-new-test.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: kubeserve-pv
  namespace: practice3
spec:
  storageClassName: local-storage
  capacity:
    storage: 100Mi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: /mnt/data

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kubeserve-pvc
  namespace: practice3
spec:
  storageClassName: local-storage
  resources:
    requests:
      storage: 10Mi
  accessModes:
  - ReadWriteOnce

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve-deployment
  namespace: practice3
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kubeserve-dep
  template:
    metadata:
      labels:
        app: kubeserve-dep
    spec:
      containers:
      - name: 'hh'
        image: linuxacademycontent/kubeserve:v1
        volumeMounts:
        - name: vol1
          mountPath: /mnt/data
      volumes:
      - name: vol1
        persistentVolumeClaim:
          claimName: kubeserve-pvc

		** if it is a statefulset, u can have multiple volumeClaimtemplates, one for each pod in the replica count.
		what then for deployments. ? as deployments dont have volumeClaimTemplates ?
		if it is a deployment, then one pvc to one pv is connected to all pods on the deployment;
		makes sense, stateful needs to have unique pods, so ud have volumeClaimTempaltes instead of pvc.
		
		$ kubectl describe pvc
		Name:          vol-pvc
		Namespace:     default
		StorageClass:  local-storage
		Status:        Bound
		Volume:        pv1
		Labels:        <none>
		Annotations:   kubectl.kubernetes.io/last-applied-configuration:
						 {"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"vol-pvc","namespace":"default"},"spec":{"accessMode...
					   pv.kubernetes.io/bind-completed: yes
					   pv.kubernetes.io/bound-by-controller: yes
		Finalizers:    [kubernetes.io/pvc-protection]
		Capacity:      100Mi
		Access Modes:  RWO
		VolumeMode:    Filesystem
		Mounted By:    stateful-576fc5f74c-j898s
					   stateful-576fc5f74c-kx5ms
					   stateful-576fc5f74c-lvbv4
					   stateful-576fc5f74c-mqphq
					   stateful-576fc5f74c-r7ztv
		Events:
		  Type    Reason                Age                From                         Message
		  ----    ------                ----               ----                         -------
		  Normal  WaitForFirstConsumer  35s (x2 over 45s)  persistentvolume-controller  waiting for first consumer to be created before binding
		cloud_user@chaitanyah3681c:~$ kubectl get pods
		NAME                        READY   STATUS    RESTARTS   AGE
		stateful-576fc5f74c-j898s   1/1     Running   0          27s
		stateful-576fc5f74c-kx5ms   1/1     Running   0          27s
		stateful-576fc5f74c-lvbv4   1/1     Running   0          27s
		stateful-576fc5f74c-mqphq   1/1     Running   0          27s
		stateful-576fc5f74c-r7ztv   1/1     Running   0          27s
		cloud_user@chaitanyah3681c:~$


		**also here, as we used hostpath as pv, it will only be same as long as two pods are on the same node. 

SECURITY	
	kubernetes security primitives:
		a service account, when created, creates a token associated to that account
		that is used by any object that uses that service account to authenticate itself
		to the api server when making requests. 

		note that u can assign a clusterrole assigned to a sa defined on one namespace; default/default is the
		sa in the default namespace; but u can assign sa in a specific namespace and assign clusterroles using bindings at a cluster level.
		tested that if a sa in defined in a certain namespace and a clusterrole is bound to it by a crb, 
		then the sa can access resources at a cluster level even if the sa is defined for a namespace.
		even if the associated user is defined using context for a certain namespace, as u have access 
		to a clusterrole using the crb, you can access resources as defined by the clusterrole at a cluster level.

		all pods by default have sa of the default namespace called default and its token added to /var/run/secrets/kubernetes.io/serviceaccounts/token
		not sure wy but i cant see any bindings with that sa. so adding the token will do what ??
		answer ) this simply adds the sa to all pods created in the namespace; if u then assign roles to the sa then the pods also get that assignment and permissions.
		
		ANSWER: the default sa is attached to each pod in that ns, but no roles are bound to it by default.
		here i created a service off of a deployment and created another pod that can curl on localhost:8080 to the service nginx. 
		first, ic couldnt as the default sa had no clusterrole bound to it; i then added the view clusterrole and then the api call works.

		** curl pod here has one container that can run curl api requets, and one pod that creates a proxy to the cluster on the pod's localhost:8080.
		
		$ cat curl-pod.yaml
		apiVersion: v1
		kind: Pod
		metadata:
		  name: curlpod
		spec:
		  containers:
		  - image: tutum/curl
			command: ["sleep","999999"]
			name: main
		  - image: linuxacademycontent/kubectl-proxy
			name: proxy
		  restartPolicy: Always

		
		root@curlpod:/# curl  localhost:8001/api/v1/namespaces/default/services/nginx
		{
		  "kind": "Status",
		  "apiVersion": "v1",
		  "metadata": {

		  },
		  "status": "Failure",
		  "message": "services \"nginx\" is forbidden: User \"system:serviceaccount:default:default\" cannot get resource \"services\" in API group \"\" in the namespace \"default\"",
		  "reason": "Forbidden",
		  "details": {
			"name": "nginx",
			"kind": "services"
		  },
		  "code": 403
		  
		  $ kubectl create clusterrolebinding default-view --clusterrole=view --serviceaccount=default:default
			clusterrolebinding.rbac.authorization.k8s.io/default-view created

		}root@curlpod:/# curl  localhost:8001/api/v1/namespaces/default/services/nginx
		{
		  "kind": "Service",
		  "apiVersion": "v1",
		  "metadata": {
			"name": "nginx",
			"namespace": "default",
			"selfLink": "/api/v1/namespaces/default/services/nginx",
			"uid": "79a24333-121d-11ea-9fcc-0af4bb207622",
			"resourceVersion": "26960",
			"creationTimestamp": "2019-11-28T20:27:19Z",
			"labels": {
			  "run": "nginx"
			}
		  },
		  "spec": {
			"ports": [
			  {
				"protocol": "TCP",
				"port": 8080,
				"targetPort": 8080
			  }
			],
			"selector": {
			  "run": "nginx"
			},
			"clusterIP": "10.96.129.124",
			"type": "ClusterIP",
			"sessionAffinity": "None"
		  },
		  "status": {
			"loadBalancer": {

			}
		  }
		}root@curlpod:/#

---

		$ kubectl get deployments -w
		NAME       READY   UP-TO-DATE   AVAILABLE   AGE
		dep-test   0/5     5            0           5s
		dep-test   1/5     5            1           5s
		dep-test   2/5     5            2           6s
		dep-test   3/5     5            3           7s
		dep-test   4/5     5            4           7s
		dep-test   5/5     5            5           8s
		^Ccloud_user@chaitanyah3681c:~/prac$ kubectl get pods
		NAME                        READY   STATUS    RESTARTS   AGE
		dep-test-64d5b449d9-dpljt   3/3     Running   0          18s
		dep-test-64d5b449d9-gpcbn   3/3     Running   0          18s
		dep-test-64d5b449d9-hg6nh   3/3     Running   0          18s
		dep-test-64d5b449d9-pv7zv   3/3     Running   0          18s
		dep-test-64d5b449d9-spspn   3/3     Running   0          18s
		pod1                        1/1     Running   0          9m40s
		pod2                        2/2     Running   1          24m
		cloud_user@chaitanyah3681c:~/prac$

		**in get deployments op, ready column gives no of replicas in a deployments; ie, pods.
		in get pods op, ready column gives containers.


		an example is shown below:
		$ kubectl get sa jenkins -o yaml
		apiVersion: v1
		kind: ServiceAccount
		metadata:
		  creationTimestamp: "2019-06-13T01:11:02Z"
		  name: jenkins
		  namespace: default
		  resourceVersion: "354803"
		  selfLink: /api/v1/namespaces/default/serviceaccounts/jenkins
		  uid: 1cb750f4-8d78-11e9-a170-0ae96c07b466
		secrets:
		- name: jenkins-token-dxsvf

		$ kubectl get secrets jenkins-token-dxsvf
		NAME                  TYPE                                  DATA   AGE
		jenkins-token-dxsvf   kubernetes.io/service-account-token   3      165d

		if no sa is provided when a pod is being created, it assumes default sa.
		once u have this account token, u can then add a kubernetes cli plugin in jenkins 
		and give this token, so jenkins server can then handle and manage these pods and can 
		also handle making those requests.

		created a user chai; created a clusterrolebinding that maps system:anonymous to 
		clusterrole cluster-admin.
		$ kubectl create clusterrolebinding cluster-admin-anonymous --clusterrole=cluster-role --user=system:anonymous
		
		$ kubectl get clusterrolebinding cluster-system-anonymous -o yaml
		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRoleBinding
		metadata:
		  creationTimestamp: "2019-06-13T01:28:17Z"
		  name: cluster-system-anonymous
		  resourceVersion: "356671"
		  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-system-anonymous
		  uid: 85528e05-8d7a-11e9-8aba-0ae96c07b466
		roleRef:
		  apiGroup: rbac.authorization.k8s.io
		  kind: ClusterRole
		  name: cluster-admin
		subjects:
		- apiGroup: rbac.authorization.k8s.io
		  kind: User
		  name: system:anonymous

		now copying the ca.crt to another remote server and trying to access the cluster
		as user chai using clusterrole cluster-admin.
		/etc/kubernetes/pki/ca.crt file over to other server's remote. 
		install kubectl if new server doesnt have it.
		
		**note that the reason u can use a user named chai is because of the clusterrolebinding created
		 called cluster-system-anonymous that binds system:anonymous to cluster-admin clusterrole.
		 i removed that and now i cant access the cluster on worker nodes 2 and 3.
		 
		 ex on 3:
		 
		 $ kubectl get nodes
		NAME                              STATUS   ROLES    AGE    VERSION
		chaitanyah3681c.mylabserver.com   Ready    master   206d   v1.14.1
		chaitanyah3682c.mylabserver.com   Ready    <none>   206d   v1.14.1
		chaitanyah3683c.mylabserver.com   Ready    <none>   206d   v1.14.1
		cloud_user@chaitanyah3683c:~$ kubectl config view
		apiVersion: v1
		clusters:
		- cluster:
			certificate-authority-data: DATA+OMITTED
			server: https://172.31.34.211:6443
		  name: kubernetes
		contexts:
		- context:
			cluster: kubernetes
			namespace: default
			user: chai
		  name: kubernetes
		current-context: kubernetes
		kind: Config
		preferences: {}
		users:
		- name: chai
		  user:
			password: password
			username: chai
		cloud_user@chaitanyah3683c:~$ kubectl get clusterrolebinding | less
		cloud_user@chaitanyah3683c:~$ kubectl get clusterrolebinding -o wide | less
		cloud_user@chaitanyah3683c:~$
		cloud_user@chaitanyah3683c:~$
		cloud_user@chaitanyah3683c:~$
		cloud_user@chaitanyah3683c:~$
		cloud_user@chaitanyah3683c:~$ kubectl delete clusterrolebinding cluster-system-anonymous
		clusterrolebinding.rbac.authorization.k8s.io "cluster-system-anonymous" deleted
		cloud_user@chaitanyah3683c:~$ kubectl get nodes
		Error from server (Forbidden): nodes is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
		cloud_user@chaitanyah3683c:~$

-------------------
		how to provide access to users in the cluster:
		using chai-csr.json as shown below created pem and key files for system:user:chai like so:
		$ cat chai-csr.json
		{
		"CN": "system:user:chai",
		"key": {
			"algo": "rsa",
			"size": 2048
			},
			"names": [
			{
			"C": "US",
			"L": "Portland",
			"O": "system:user:chai",
			"OU": "kube",
			"ST": "Oregon"
			}
			]
		}
		cloud_user@chaitanyah3685c:~/kube-cluster-prep$ cat ca-config.json
		{
		"signing": {
			"default": {
			"expiry": "8760h"
			},
			"profiles": {
				"kubernetes": {
				"usages": ["signing","key encipherment","server auth","client auth"],
				"expiry": "8760h"
				}
				}
			}
		}

		cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes chai-csr.json | cfssljson -bare chai
		chai-csr.json
		chai.pem
		chai-key.pem
		chai.csr

		kc config set-credentials chai --client-certificate=chai.pem --client-key=chai-key.pem --embed-certs=true
		kc config set-context chai-user --cluster=kube --user=chai

		kc create clusterrolebinding chai-user-edit-role --clusterrole=view  --user=system:user:chai

		and now u can use system:user:chai for accessing the cluster.
		$ kc config get-contexts
		CURRENT   NAME        CLUSTER   AUTHINFO   NAMESPACE
		*         chai-user   kube      chai
				default     kube      admin
		cloud_user@chaitanyah3685c:~/kube-cluster-prep$ kc get pods
		NAME      READY   STATUS    RESTARTS   AGE
		busybox   1/1     Running   6          8d
		curlpod   2/2     Running   2          27h


--------------------

		now adding user view level access to worker nodes 2 and 3 when accessed using user chai.
			using clusterrole called view.
			
			$ kubectl create clusterrolebinding sa-chai-view --serviceaccount=default:sa-chai --clusterrole=view
			clusterrolebinding.rbac.authorization.k8s.io/sa-chai-view created
			cloud_user@chaitanyah3681c:~/test-prep$ kubectl get sa
			NAME      SECRETS   AGE
			default   1         206d
			jenkins   1         166d
			sa-chai   1         6m21s
			cloud_user@chaitanyah3681c:~/test-prep$ kubectl get clusterrolebindings | grep chai
			sa-chai-view                                           17s

			
			once done, create using set-cluster and set-credentials the content in config file on both worker nodes. 
			
			$ kubectl config set-cluster kubernetes --server=https://172.31.43.8:6443 --certificate-authority=ca.crt --embed-certs=true
			Cluster "kubernetes" set.


			** u have to use kubectl describe secret instead of kubectl get secret -o yaml; gives diff token encoded string that doesnt work.
			# get the token from kubectl describe secret <name of the secret of the sa> 
			$ kubectl config set-credentials chai --token=eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InNhLWNoYWktdG9rZW4ta3B4eHMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoic2EtY2hhaSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImNjZjYzOWMxLTEwYzQtMTFlYS1hYTI0LTBhZjRiYjIwNzYyMiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnNhLWNoYWkifQ.bmYCn9ZQPZGDkxBHvez7pIgmCHJ_WVA9Ga5h_E_mjlZ7tbRLttVt_m02F3syQwk_810KIWMcGyhvlQ3BKBTx_dlbOtMlNVRSt4HpFSt3wdNkVS6OgmGSAwjtqOkSozzK8-9lxVKZvwaOhUrctQ7MkV7B-KTWdcUvb8NoGZRudUaOU2sLbIsjOQhc6P9A5c6nWXfTVVCdWxsEQ7ALRNpvk9VPjFR184qHp17PDSkAy0blDvnGIiF7tzv9qomU3vy8B3WnPzygxyzkn0H5VFy0tFbCo4AEpjrRV1Zlip7BHL0UFR6ptOvTLZtzISAaWhW1OgGySSdy0o22jYmWa3vZ0Q

			$ kubectl config set-context sa-chai-view-context --user=chai --cluster=kubernetes
			Context "sa-chai-view-context" created.

			$ kubectl config use-context sa-chai-view-context
			Switched to context "sa-chai-view-context".

			cloud_user@chaitanyah3681c:/tmp$ kubectl config view
			apiVersion: v1
			clusters:
			- cluster:
				certificate-authority-data: DATA+OMITTED
				server: https://172.31.43.8:6443
			  name: kubernetes
			contexts:
			- context:
				cluster: kubernetes
				user: chai
			  name: sa-chai-view-context
			current-context: sa-chai-view-context
			kind: Config
			preferences: {}
			users:
			- name: chai
			  user:
				token: sa-chai-token-kpxxs
				
			**kubectl needs to be max one minor version less than the server version to use kubectl.
			if not, this error is seen:
			
			$ kubectl get pods
			error: the server doesn't have a resource type "pods"

			s per https://kubernetes.io/docs/tasks/tools/install-kubectl/#before-you-begin:

			You must use a kubectl version that is within one minor version difference of your cluster. For example, a v1.2 client should work with v1.1, v1.2, and v1.3 master. Using the latest version of kubectl helps avoid unforeseen issues.


			$ kubectl get pods --all-namespaces
			NAMESPACE     NAME                                                      READY   STATUS    RESTARTS   AGE
			kube-system   coredns-fb8b8dccf-mqpkl                                   1/1     Running   1          165m
			kube-system   coredns-fb8b8dccf-tj7cd                                   1/1     Running   1          165m
			kube-system   etcd-chaitanyah3684c.mylabserver.com                      1/1     Running   0          144m
			kube-system   kube-apiserver-chaitanyah3684c.mylabserver.com            1/1     Running   0          144m
			kube-system   kube-controller-manager-chaitanyah3684c.mylabserver.com   1/1     Running   0          144m
			kube-system   kube-flannel-ds-amd64-7vxgk                               1/1     Running   2          170m
			kube-system   kube-flannel-ds-amd64-8dbhg                               1/1     Running   1          35m
			kube-system   kube-flannel-ds-amd64-zvm59                               1/1     Running   1          35m
			kube-system   kube-proxy-bflj6                                          1/1     Running   1          35m
			kube-system   kube-proxy-sbnmd                                          1/1     Running   1          165m
			kube-system   kube-proxy-vmkls                                          1/1     Running   1          35m
			kube-system   kube-scheduler-chaitanyah3684c.mylabserver.com            1/1     Running   0          144m
			cloud_user@chaitanyah3681c:/tmp$


			****updating sa to be in office namespace and testing if the view clusterrole allows it
			to access other resources in other namespaces too.


			$ kubectl set -h
			Configure application resources

			 These commands help you make changes to existing application resources.

			Available Commands:
			  env            Update environment variables on a pod template
			  image          Update image of a pod template
			  resources      Update resource requests/limits on objects with pod templates
			  selector       Set the selector on a resource
			  serviceaccount Update ServiceAccount of a resource
			  subject        Update User, Group or ServiceAccount in a RoleBinding/ClusterRoleBinding

			Usage:
			  kubectl set SUBCOMMAND [options]
			  
			** i created two deployments one in default and one in another ns, i see both deployments get created
			with the same rs.
			cloud_user@chaitanyah3684c:~/test-prep$ kubectl get rs --all-namespaces
			NAMESPACE     NAME                 DESIRED   CURRENT   READY   AGE
			default       nginx-7db9fccd9b     2         2         2       85s
			kube-system   coredns-54ff9cd656   0         0         0       43h
			kube-system   coredns-fb8b8dccf    2         2         2       42h
			office        nginx-7db9fccd9b     2         2         2       61s

			but there are 4 pods with the same rs tho:
			$ kubectl get pods --all-namespaces | grep nginx
			default       nginx-7db9fccd9b-bsd96                                    1/1     Running   0          3m5s
			default       nginx-7db9fccd9b-fnlx7                                    1/1     Running   0          3m5s
			office        nginx-7db9fccd9b-8qf5p                                    1/1     Running   0          2m41s
			office        nginx-7db9fccd9b-rfchd      

			so on using a token created for a sa in a namespace as authentication for the user, 
			i see that i can get pods both in the namespace of the sa and also in the default ns.

-------------------		

			
			
		**CONFIGURING USER FOR YOUR CLUSTER:
			USING CERTS AND BY USING SERVICEACCOUNT'S TOKEN
			
				**READ UP THIS GUIDE; PRETTY USEFUL WAY OF GETTING RBAC WORKING IN KUBERNETES.
				In this guide you can find how to configure a user for your cluster: https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/#use-case-1-create-user-with-limited-namespace-access

				--- 
				WORKING IN TEST ENVIRONMENT:
				$ openssl genrsa -out employee.key 2048
				
				$ openssl req -new -key employee.key -out employee.csr -subj "/CN=employee/O=office-grp"

				
				$ sudo openssl x509 -req -in employee.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out employee.crt -days 3650
				Signature ok
				subject=CN = employee, O = office-grp
				Getting CA Private Key
				
				cloud_user@chaitanyah3684c:~$ ls -ltr
				total 205760
				drwxr-xr-x 2 cloud_user cloud_user      4096 Feb 22  2019 Videos
				drwxr-xr-x 2 cloud_user cloud_user      4096 Feb 22  2019 Templates
				drwxr-xr-x 2 cloud_user cloud_user      4096 Feb 22  2019 Public
				drwxr-xr-x 2 cloud_user cloud_user      4096 Feb 22  2019 Pictures
				drwxr-xr-x 2 cloud_user cloud_user      4096 Feb 22  2019 Music
				drwxr-xr-x 2 cloud_user cloud_user      4096 Feb 22  2019 Downloads
				drwxr-xr-x 2 cloud_user cloud_user      4096 Feb 22  2019 Documents
				drwxr-xr-x 2 cloud_user cloud_user      4096 Feb 22  2019 Desktop
				-rw-rw-r-- 1 cloud_user cloud_user  39587104 Nov 27 00:48 kubeadm
				-rw-rw-r-- 1 cloud_user cloud_user 127940544 Nov 27 01:11 kubelet
				-rw-rw-r-- 1 cloud_user cloud_user  43115328 Nov 27 01:13 kubectl
				-rw------- 1 cloud_user cloud_user      1679 Nov 27 03:51 employee.key
				-rw-rw-r-- 1 cloud_user cloud_user       920 Nov 27 04:05 employee.csr
				-rw-r--r-- 1 root       root            1021 Nov 27 04:06 employee.crt
				
				cloud_user@chaitanyah3684c:~$ cat employee.crt
				-----BEGIN CERTIFICATE-----
				MIICxDCCAawCFGoetCVVuWJHxVRiGTbpGXAFS7m3MA0GCSqGSIb3DQEBCwUAMBUx
				EzARBgNVBAMTCmt1YmVybmV0ZXMwHhcNMTkxMTI3MDQwNjQxWhcNMjkxMTI0MDQw
				NjQxWjAoMREwDwYDVQQDDAhlbXBsb3llZTETMBEGA1UECgwKb2ZmaWNlLWdycDCC
				ASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKue2mhU48uvHDtCn/AJZuxO
				KqezJvWwfR1NV7o1Kx3CLeQIjesb4dujGwa9/6SObn5wZVqwYsybaQRrAmTGCNkr
				zDESB8cOs47sI4QMsruHUaNoFyEa5v8i2pAl9Z6tdMEC5uvaWj7AUnLEwywm508p
				JIV0CWeSW5b+ROyNq34MNGaPge12+qJikLeQe//pKyNY0DVMZF4lzkWYdw1wmiSf
				e3zfWDrpc3lwH2n0P0CsBkByzzmkm7v1yaiL4lwKeV68ml/tIKr28sUBJUJIpii4
				54xzjj0pVDqPY2SGtV7UPr5EBWHgDM8L62mB7UHkeYWBw3xE+qi42xFHoBU5X9MC
				AwEAATANBgkqhkiG9w0BAQsFAAOCAQEABz9xR0E9xrq3fEGri11L4rJmU1uQGT9o
				ETfWnjU3uyC7ThGwLnNTDvtI6f6U6PsWLASkCj33yTO21AJPUN7MwDPB0VnAR8or
				f/vHHEuvh9kBz8t+JGPQFGlkV4N8hxfvIqK8pO4KlxAisfsjK2iqdjIRjUDloP7j
				zabv5QNzQ6wenboWToMiab4yie11B82rbQGEib4aKVIUmvJ5AJ/wEdtI0eMvBW0L
				ZSeBSS54KzebxM2S70opvaM1ntJe8Q88E0NpQlf4zdf9KzW+W1kGDGq9XgOeQoEc
				cBWDvrR5j9WOyE66ZzarSX9LZoP+KSZBjevSRRkDGlJVnoZY61hMdg==
				-----END CERTIFICATE-----

				save the .key and .crt files and create credentials with those two
				
				$ kubectl config set-credentials employee --client-certificate=/home/cloud_user/employee.crt --client-key=/home/cloud_user/employee.key --embed-certs=true
				User "employee" set.

				$ kubectl config set-context employee-office --user=employee --namespace=office --cluster=kubernetes
				Context "employee-office" created.
				cloud_user@chaitanyah3682c:~$ kubectl config view
				apiVersion: v1
				clusters:
				- cluster:
					certificate-authority-data: DATA+OMITTED
					server: https://172.31.43.8:6443
				  name: kubernetes
				contexts:
				- context:
					cluster: kubernetes
					namespace: office
					user: employee
				  name: employee-office
				- context:
					cluster: kubernetes
					user: chai
				  name: sa-chai-view-context
				current-context: sa-chai-view-context
				kind: Config
				preferences: {}
				users:
				- name: chai
				  user:
					token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InNhLWNoYWktdG9rZW4ta3B4eHMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoic2EtY2hhaSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImNjZjYzOWMxLTEwYzQtMTFlYS1hYTI0LTBhZjRiYjIwNzYyMiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnNhLWNoYWkifQ.bmYCn9ZQPZGDkxBHvez7pIgmCHJ_WVA9Ga5h_E_mjlZ7tbRLttVt_m02F3syQwk_810KIWMcGyhvlQ3BKBTx_dlbOtMlNVRSt4HpFSt3wdNkVS6OgmGSAwjtqOkSozzK8-9lxVKZvwaOhUrctQ7MkV7B-KTWdcUvb8NoGZRudUaOU2sLbIsjOQhc6P9A5c6nWXfTVVCdWxsEQ7ALRNpvk9VPjFR184qHp17PDSkAy0blDvnGIiF7tzv9qomU3vy8B3WnPzygxyzkn0H5VFy0tFbCo4AEpjrRV1Zlip7BHL0UFR6ptOvTLZtzISAaWhW1OgGySSdy0o22jYmWa3vZ0Q
				- name: employee
				  user:
					client-certificate-data: REDACTED
					client-key-data: REDACTED

				created clusterrole of view to user employee. 
				make note this is at a cluster level. but the context was initially set to office namespace only.
				doesnt matter, just wanted to point out.
				$ kubectl create clusterrolebinding employee-office-view --user=employee --clusterrole=view

				$ kubectl config use-context employee-office
				Switched to context "employee-office".
				cloud_user@chaitanyah3682c:~$ kubectl config current-context
				employee-office
				cloud_user@chaitanyah3682c:~$ kubectl get pods --namespace kube-system
				NAME                                                      READY   STATUS    RESTARTS   AGE
				coredns-fb8b8dccf-mqpkl                                   1/1     Running   1          3h29m
				coredns-fb8b8dccf-tj7cd                                   1/1     Running   1          3h29m
				etcd-chaitanyah3684c.mylabserver.com                      1/1     Running   0          3h8m
				kube-apiserver-chaitanyah3684c.mylabserver.com            1/1     Running   0          3h8m
				kube-controller-manager-chaitanyah3684c.mylabserver.com   1/1     Running   0          3h8m
				kube-flannel-ds-amd64-7vxgk                               1/1     Running   2          3h34m
				kube-flannel-ds-amd64-8dbhg                               1/1     Running   1          78m
				kube-flannel-ds-amd64-zvm59                               1/1     Running   1          78m
				kube-proxy-bflj6                                          1/1     Running   1          78m
				kube-proxy-sbnmd                                          1/1     Running   1          3h29m
				kube-proxy-vmkls                                          1/1     Running   1          78m
				kube-scheduler-chaitanyah3684c.mylabserver.com            1/1     Running   0          3h8m
				cloud_user@chaitanyah3682c:~$




				Long story short:

					Create rsa key for the user
						openssl genrsa -out employee.key 2048
					Create a certificate sign request using key
						openssl req -new -key employee.key -out employee.csr -subj "/CN=employee/O=office-grp"
					Sign the certificate with the cluster certificate authority
						openssl x509 -req -in employee.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out employee.crt -days 3650
					Create a configuration for your user - crt and key files needed.
						kubectl config set-credentials user --client-certificate=/crt --client-key=/key --embed-certs=true
					Add RBAC rules for this user or its group
						kubectl create clusterrolebinding crbrule --user=user --clusterrole=clusterrole.

				Regarding the ca.crt, you need to find it in your master host.

				Edited: In the case of GKE, check here https://cloud.google.com/container-engine/docs/iam-integration
			
				name of the user in ckubeconfig is unnecessary; name provided CN and name of the files .key and crt represent the user here. 
				i named user in kubeconfig as chai-cert but i defiend the name during cert creation as chai and chai was set as user in crb to allow access
				to the cluster for that user.
			
---------------------------------------

				**THE SAME BY EMBEDDING USER/PASSWORDS AS SECRETS
				**NEED TO DEBUG THIS ONE THO; NOT WORKING AS OF NOW; PICKED UP
				FROM AN ONLINE FORUM AS A QUESTION.
				I need to create users to assign them permissions with RBAC, I create them as follows:

				echo -n "lucia" | base64
				bHVjaWE=
				echo -n "pass" | base64
				cGFzcw==

				apiVersion: v1
				kind: Secret
				metadata:
				  name: lucia-secret
				type: Opaque
				data:
				  username: bHVjaWE=
				  password: cGFzcw==

				Or create with:

				kubectl create secret generic lucia-secret --from-literal=username='lucia',password='pass'

				I don't know how to continue

				USER_NICK=lucia

				kubectl config set-credentials $USER_NICK \
					--username=lucia \
					--password=pass

				kubectl get secret lucia-secret -o json | jq -r '.data["ca.crt"]' | base64 -d > ca.crt

				endpoint=`kubectl config view -o jsonpath="{.clusters[?(@.name == \"$name\")].cluster.server}"`

				kubectl config set-cluster cluster-for-lucia \
				  --embed-certs=true \
				  --server=$endpoint \
				  --certificate-authority=./ca.crt

				kubectl config set-context context-lucia \
				  --cluster=cluster-for-lucia \
				  --user=$USER_NICK \
				  --namespace=default


		
		trying to use the other worker node as a remote server from which to access
		kubectl config view doesnt have this cluster set. 
		so we can set the cluster, set credentials and set context to use the credentials for the cluster we have.
		
		$ kubectl config set-cluster kubernetes --server=https://172.31.34.211:6443 --certificate-authority=ca.crt --embed-certs=true
		Cluster "kubernetes" set.
		cloud_user@chaitanyah3683c:/etc/kubernetes/pki$ kubectl config view
		apiVersion: v1
		clusters:
		- cluster:
			certificate-authority-data: DATA+OMITTED
			server: https://172.31.34.211:6443
		  name: kubernetes
		contexts: []
		current-context: ""
		kind: Config
		preferences: {}
		users: []

		$ kubectl config set-credentials chai --username=chai --password=password
		User "chai" set.
		cloud_user@chaitanyah3683c:/etc/kubernetes/pki$ kubectl config set-context kubernetes --cluster=kubernetes --user=chai --namespace=default
		Context "kubernetes" created.
		cloud_user@chaitanyah3683c:/etc/kubernetes/pki$ kubectl config use-context kubernetes
		Switched to context "kubernetes".
		cloud_user@chaitanyah3683c:/etc/kubernetes/pki$ kubectl config view   
		apiVersion: v1
		clusters:
		- cluster:
			certificate-authority-data: DATA+OMITTED
			server: https://172.31.34.211:6443
		  name: kubernetes
		contexts:
		- context:
			cluster: kubernetes
			namespace: default
			user: chai
		  name: kubernetes
		current-context: kubernetes
		kind: Config
		preferences: {}
		users:
		- name: chai
		  user:
			password: password
			username: chai

		contexts are used to connect a user to a cluster; can be used to 
		connect to multiple clusters - also used in the exam.
		once u create a context u need to use the context to be able to access teh api server form the remote server.
		
		**IMP good practive to create a service account for each pod in a cluster and then associate that pod to a clusterrole
		using a clusterrolebinding.
		

		Authentication documentation:
		https://kubernetes.io/docs/tasks/administer-cluster/certificates/
		https://kubernetes.io/docs/reference/access-authn-authz/authentication/

		---
		
		Expanding on our discussion about securing the Kubernetes cluster, well take a look at service accounts and user authentication. Also in this lesson, we will create a workstation for you to administer your cluster without logging in to the Kubernetes master server.

		List the service accounts in your cluster:

		kubectl get serviceaccounts

		Create a new jenkins service account:

		kubectl create serviceaccount jenkins

		Use the abbreviated version of serviceAccount:

		kubectl get sa

		View the YAML for our service account:

		kubectl get serviceaccounts jenkins -o yaml

		View the secrets in your cluster:

		kubectl get secret [secret_name]

		The YAML for a busybox pod using the jenkins service account:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: busybox
		  namespace: default
		spec:
		  serviceAccountName: jenkins
		  containers:
		  - image: busybox:1.28.4
			command:
			  - sleep
			  - "3600"
			imagePullPolicy: IfNotPresent
			name: busybox
		  restartPolicy: Always

		Create a new pod with the service account:

		kubectl apply -f busybox.yaml

		View the cluster config that kubectl uses:

		kubectl config view

		View the config file:

		cat ~/.kube/config

		Set new credentials for your cluster:

		kubectl config set-credentials chad --username=chad --password=password

		Create a role binding for anonymous users (not recommended):

		kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=system:anonymous

		SCP the certificate authority to your workstation or server:

		scp ca.crt cloud_user@[pub-ip-of-remote-server]:~/

		Set the cluster address and authentication:

		kubectl config set-cluster kubernetes --server=https://172.31.41.61:6443 --certificate-authority=ca.crt --embed-certs=true

		Set the credentials for Chad:

		kubectl config set-credentials chad --username=chad --password=password

		Set the context for the cluster:

		kubectl config set-context kubernetes --cluster=kubernetes --user=chad --namespace=default

		Use the context:

		kubectl config use-context kubernetes

		Run the same commands with kubectl:

		kubectl get nodes

	authentication and authorization:
		roles
		clusterroles
		
		rolebinding
		clusterrolebinding
		
		role/rolebindings are namespaced; clsuterrole/crb are cluster level
		
		role/clusterrole to define what accesses are provided.
		bindings are to bind them to an endpoint - user or serviceaccounts - objects.
		
		created a role that has access to read service endpoint in web namespace:
		$ cat role.yaml
		apiVersion: rbac.authorization.k8s.io/v1
		kind: Role
		metadata:
		  namespace: web
		  name: service-reader
		rules:
		- apiGroups: [""]
		  verbs: ["get","list"]
		  resources: ["services"]

		rolebinding always mentions a single role, but can bind that role
		to multiple serviceaccounts, user or groups of users.
		
		binding the role to default sa in web namesapce:
		$ kubectl create rolebinding test --role=service-reader --serviceaccount=web:default -n web
		rolebinding.rbac.authorization.k8s.io/test created

		//** created a rolebinding that maps default sa to that role. 
		created a curlpod that creats kubectl proxy to the apiserver and also can curl into the api server
		on localhost:8001, this can now get and list on the services in the default ns.
		
		$ kubectl get roles -o yaml
		apiVersion: v1
		items:
		- apiVersion: rbac.authorization.k8s.io/v1
		  kind: Role
		  metadata:
			annotations:
			  kubectl.kubernetes.io/last-applied-configuration: |
				{"apiVersion":"rbac.authorization.k8s.io/v1","kind":"Role","metadata":{"annotations":{},"name":"service-read-role","namespace":"default"},"rules":[{"apiGroups":[""],"resources":["services"],"verbs":["get","list"]}]}
			creationTimestamp: "2019-11-28T20:53:30Z"
			name: service-read-role
			namespace: default
			resourceVersion: "29627"
			selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/default/roles/service-read-role
			uid: 21e78eb5-1221-11ea-9fcc-0af4bb207622
		  rules:
		  - apiGroups:
			- ""
			resources:
			- services
			verbs:
			- get
			- list
		kind: List
		metadata:
		  resourceVersion: ""
		  selfLink: ""
		cloud_user@chaitanyah3684c:~/test-prep$ kubectl get rolebinding -o yaml
		apiVersion: v1
		items:
		- apiVersion: rbac.authorization.k8s.io/v1
		  kind: RoleBinding
		  metadata:
			creationTimestamp: "2019-11-28T20:54:18Z"
			name: default-sa-read-services
			namespace: default
			resourceVersion: "29328"
			selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/default/rolebindings/default-sa-read-services
			uid: 3ee2bb80-1221-11ea-9fcc-0af4bb207622
		  roleRef:
			apiGroup: rbac.authorization.k8s.io
			kind: Role
			name: service-read-role
		  subjects:
		  - kind: ServiceAccount
			name: default
			namespace: default
		kind: List
		metadata:
		  resourceVersion: ""
		  selfLink: ""


		this allows sa web to gain access of those api requests mentioned in the role definition.
		using kubectl proxy to open up cluster communication locally on port 8001
		
		for clusterrole/clusterrolebinding:
		 kubectl get clusterrole pv-reader -o yaml
		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRole
		metadata:
		  creationTimestamp: "2019-06-13T01:53:50Z"
		  name: pv-reader
		  resourceVersion: "359280"
		  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/pv-reader
		  uid: 1759534b-8d7e-11e9-8aba-0ae96c07b466
		rules:
		- apiGroups:
		  - ""
		  resources:
		  - persistentvolumes
		  verbs:
		  - get
		  - list

		creating a clusterrolebinding to give access to default service account of web namespace
		access to reading pv info on the cluster.
		
		$ kubectl get clusterrolebinding pv-test -o yaml
		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRoleBinding
		metadata:
		  creationTimestamp: "2019-06-13T02:06:17Z"
		  name: pv-test
		  resourceVersion: "360561"
		  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/pv-test
		  uid: d45fe3c8-8d7f-11e9-8aba-0ae96c07b466
		roleRef:
		  apiGroup: rbac.authorization.k8s.io
		  kind: ClusterRole
		  name: pv-reader
		subjects:
		- kind: ServiceAccount
		  name: default
		  namespace: web

		now, to test this, lets create a pod in web namespace that curls to the cluster.
		$ cat curl-pod.yaml
		apiVersion: v1
		kind: Pod
		metadata:
		  namespace: web
		  name: curlpod
		spec:
		  containers:
		  - name: main
			image: radial/busyboxplus:curl
			command: ["sleep","999999"]
		  - name: proxy
			image: linuxacademycontent/kubectl-proxy
		  restartPolicy: Always

		now, this pod should be able to issue curl requests to get/list pv related information at the cluster level.
		on getting into the pod (main container) and running curl on getting pv info; the curl wokrs:
		
		[ root@curlpod:/ ]$ curl localhost:8001/api/v1/persistentvolumes
		{
		  "kind": "PersistentVolumeList",
		  "apiVersion": "v1",
		  "metadata": {
			"selfLink": "/api/v1/persistentvolumes",
			"resourceVersion": "2147964"
		  },
		  "items": [
			{
			  "metadata": {
				"name": "data-pv",
				"selfLink": "/api/v1/persistentvolumes/data-pv",
				"uid": "84f78f0c-033d-11ea-8b7a-0ae96c07b466",
				"resourceVersion": "2014535",
				"creationTimestamp": "2019-11-09T22:08:54Z",
				"annotations": {
				  "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolume\",\"metadata\":{\"annotations\":{},\"name\":\"data-pv\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"capacity\":{\"storage\":\"100Mi\"},\"hostPath\":{\"path\":\"/mnt/data2\"},\"storageClassName\":\"local-storage\"}}\n",
				  "pv.kubernetes.io/bound-by-controller": "yes"
				},
				"finalizers": [
				  "kubernetes.io/pv-protection"
				]
			  },
			  "spec": {
				"capacity": {
				  "storage": "100Mi"
				},
				"hostPath": {
				  "path": "/mnt/data2",
				  "type": ""
				},
				"accessModes": [
				  "ReadWriteOnce"
				],
				"claimRef": {
				  "kind": "PersistentVolumeClaim",
				  "namespace": "web",
				  "name": "data-pvc",
				  "uid": "c1f2297a-033d-11ea-8b7a-0ae96c07b466",
				  "apiVersion": "v1",
				  "resourceVersion": "2013221"
				},
				"persistentVolumeReclaimPolicy": "Retain",
				"storageClassName": "local-storage",
				"volumeMode": "Filesystem"
			  },
			  "status": {
				"phase": "Released"
			  }
			}
		  ]

		keep in mind the other linuxacademycontent/kubectl-proxy pod was used to enable internal cluster comm on the pod for the main container to be able to do a query on localhost:8001
		
		**create sa; use token for user authentication and authorization in kubeconfig using --token in set-credentials command
		assing a sa to the pod created; if not ns:default sa is attached to the pod.
		create .key; .csr usign .key; create .crt using .csr and ca of the master. use both in --client-certificates and --client-key in set-credentials.
		then create a role or clusterrolebinding on the user name used while creating the cert.
		
		Once the API server has determined who you are (whether a pod or a user), the authorization is handled by RBAC. In this lesson, we will talk about roles, cluster roles, role bindings, and cluster role bindings.

		Create a new namespace:

		kubectl create ns web

		The YAML for a service role:

		apiVersion: rbac.authorization.k8s.io/v1
		kind: Role
		metadata:
		  namespace: web
		  name: service-reader
		rules:
		- apiGroups: [""]
		  verbs: ["get", "list"]
		  resources: ["services"]

		Create a new role from that YAML file:

		kubectl apply -f role.yaml

		Create a RoleBinding:

		kubectl create rolebinding test --role=service-reader --serviceaccount=web:default -n web

		Run a proxy for inter-cluster communications:

		kubectl proxy

		Try to access the services in the web namespace:

		curl localhost:8001/api/v1/namespaces/web/services

		Create a ClusterRole to access PersistentVolumes:

		kubectl create clusterrole pv-reader --verb=get,list --resource=persistentvolumes

		Create a ClusterRoleBinding for the cluster role:

		kubectl create clusterrolebinding pv-test --clusterrole=pv-reader --serviceaccount=web:default

		The YAML for a pod that includes a curl and proxy container:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: curlpod
		  namespace: web
		spec:
		  containers:
		  - image: tutum/curl
			command: ["sleep", "9999999"]
			name: main
		  - image: linuxacademycontent/kubectl-proxy
			name: proxy
		  restartPolicy: Always

		Create the pod that will allow you to curl directly from the container:

		kubectl apply -f curl-pod.yaml

		Get the pods in the web namespace:

		kubectl get pods -n web

		Open a shell to the container:

		kubectl exec -it curlpod -n web -- sh

		Access PersistentVolumes (cluster-level) from the pod:

		curl localhost:8001/api/v1/persistentvolumes

		RBAC TESTING:
		**TRYING TO CREATE A NAMESPCE FOR AN APP, CREATING A ROLE THAT ACTS LIKE AN ADMIN IN THAT
		NAMESPACE, ASSIGN IT TO A NEW SA AND PROVIDE THAT TO APP TEAMS AS TOKEN USED TO LOGIN INTO THE CLUSTER.

		kubectl api-resources --namespaced=true to get all the resources that can be used within a namespace.
		put all of those as resources in the below yaml that creates a namespace admin.

		$ cat namespce-admin.yaml
		apiVersion: rbac.authorization.k8s.io/v1
		kind: Role
		metadata:
		  name: app-admin
		  namespace: app
		rules:
		- apiGroups: ["*"]
		  verbs: ["*"]
		  resources: ["bindings","configmaps","endpoints","events","limitranges","persistentvolumeclaims","pods","podtemplates","replicationcontrollers","resourcequotas","secrets","serviceaccounts","services","controllerrevisions","daemonsets","deployments","replicasets","statefulsets","localsubjectaccessreviews","horizontalpodautoscalers","cronjobs","jobs","leases","networkpolicies","events","daemonsets","deployments","ingresses","networkpolicies","replicasets","ingresses","networkpolicies","poddisruptionbudgets","rolebindings","roles"]

		$ kubectl get role -n app
		NAME        AGE
		app-admin   4m13s

		as default gets assigned to pods within the namespace app, created another sa called admin in app namespace.

		$ kubectl get sa -n app
		NAME      SECRETS   AGE
		admin     1         3m1s
		default   1         16m

		created a role binding that maps service account app:admin to role app-admin.
		*make sure u create the role binding in that namespace app.

		$ kubectl get rolebinding app-admin-rb -o yaml
		apiVersion: rbac.authorization.k8s.io/v1
		kind: RoleBinding
		metadata:
		  creationTimestamp: "2019-11-28T22:01:40Z"
		  name: app-admin-rb
		  namespace: app
		  resourceVersion: "35363"
		  selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/default/rolebindings/app-admin-rb
		  uid: a7cc296b-122a-11ea-9fcc-0af4bb207622
		roleRef:
		  apiGroup: rbac.authorization.k8s.io
		  kind: Role
		  name: app-admin
		subjects:
		- kind: ServiceAccount
		  name: admin
		  namespace: app

		ill now use one of the worker nodes as app teams' remote node they use to connect to the cluster.
		context on the worker node:
		- context:
			cluster: kubernetes
			namespace: app
			user: app-admin
		  name: app-admin-app-ns-context

		user definition on the worker node:
		- name: app-admin
		  user:
			token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJhcHAiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiYWRtaW4tdG9rZW4tZHRraHIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiYWRtaW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI3MTJkY2MyMi0xMjJhLTExZWEtOWZjYy0wYWY0YmIyMDc2MjIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6YXBwOmFkbWluIn0.O5EXo44XSG_1ofx0MrgfYNBD2m6T6jU0wr8UDU2xzKCR5Y9MGsyEi-QUQA7hG2fBig4Sf3NLuCIZpUFeQBhWapAIskfZXxX-VQWZjaGTpjXaWw0_NCHSIX_3TDwWtQyY2bA3ttJAlgw9-aQUBQzPECEq6_I82TR2P8phtDDa_HTiyl9ij6hwUkiciSzFA29BXECpgdmFX07CspuPHEQRA79wB2n2mwOUHqm5ZmpwaqLjYERQ5oOcggK_kbKNMD8e0MnRuRE3YI0GhYX2zdIruMzSKrxoUSwu0xU2gKu3WVq4djRV-oNwXDQgXrvbxmhC9sZY9QZBzMe7zn4um0gWnQ

		on adding it, now only on the namespaced=true resources admin user has complete access to:

		$ kubectl get pods
		No resources found.
		cloud_user@chaitanyah3682c:/$ kubectl get pods --all-namespaces
		Error from server (Forbidden): pods is forbidden: User "system:serviceaccount:app:admin" cannot list resource "pods" in API group "" at the cluster scope
		cloud_user@chaitanyah3682c:/$ kubectl get roles
		NAME        AGE
		app-admin   40m
		cloud_user@chaitanyah3682c:/$ kubectl get roles --all-namespaces
		Error from server (Forbidden): roles.rbac.authorization.k8s.io is forbidden: User "system:serviceaccount:app:admin" cannot list resource "roles" in API group "rbac.authorization.k8s.io" at the cluster scope
		cloud_user@chaitanyah3682c:/$ kubectl get pv
		Error from server (Forbidden): persistentvolumes is forbidden: User "system:serviceaccount:app:admin" cannot list resource "persistentvolumes" in API group "" at the cluster scope
		cloud_user@chaitanyah3682c:/$
 

		
	#####HERE#####network policies:
	
	** kubectl proxy only creates a listening connection on localhost:8001, not on the pods ip or the node's ip.
	
		network policies allow pods to communicate with each other.
		ingress rules
		egress rules
		cidr block can be also mentioned with a rule
		canal pluign needed and will be installed for the exam.,
		without canal plugin, net policies are not effective.
		
		$ kubectl get netpol
		NAME       POD-SELECTOR   AGE
		deny-all   <none>         7s
		cloud_user@chaitanyah3681c:~/practice-7$ cat deny-netpol.yaml
		apiVersion: networking.k8s.io/v1
		kind: NetworkPolicy
		metadata:
		  name: deny-all
		spec:
		  podSelector: {}

		created a deployment and exposed it with a service, and without the deny-all
		netpol another pod can access the deployment using the service.
		
		 # wget --spider --timeout=1 nginx
		Connecting to nginx (10.102.41.194:80)
		remote file exists
		/ #

		--spider only used to get back response and not download any file.
		
		with the deny-all netpol created, it cant access the service anymore. 

		** this is so only app pods with app=web label can access app=db pods on port 5432.
		$ cat db-netpol.yaml
		apiVersion: networking.k8s.io/v1
		kind: NetworkPolicy
		metadata:
		  name: db-netpol
		spec:
		  podSelector:
			matchLabels:
			  app: db
		  ingress:
		  - from:
			- podSelector:
				matchLabels:
				  app: web
			ports:
			- port: 5432

		similarly, using namespace selector u can have pods in one namespace access pods labelled app=db on port 5432.
		
		$ cat ns-netpol.yaml
		apiVersion: networking.k8s.io/v1
		kind: NetworkPolicy
		metadata:
		  name: ns-netpol
		spec:
		  podSelector:
			matchLabels:
			  app: db
		  ingress:
		  - from:
			- namespaceSelector:
				matchLabels:
				  tenant: web
			ports:
			- port: 5432

		using ipblock as source of the request:
		$ cat ipblock-netpol.yaml
		apiVersion: networking.k8s.io/v1
		kind: NetworkPolicy
		metadata:
		  name: ns-netpol
		spec:
		  podSelector:
			matchLabels:
			  app: db
		  ingress:
		  - from:
			- ipBlock:
				cidr: 192.168.1.0/24

		egress rules similarly on app=web pods to app=db pods:
		$ cat egress-netpol.yaml
		apiVersion: networking.k8s.io/v1
		kind: NetworkPlicy
		metadata:
		  name: egress-netpol
		spec:
		  podSelector:
			matchLabels:
			  app: web
		  egress:
		  - to:
			- podSelector:
				matchLabels:
				  app: db
			ports:
			- port: 5432

---

		Network policies allow you to specify which pods can talk to other pods. This helps when securing communication between pods, allowing you to identify ingress and egress rules. You can apply a network policy to a pod by using pod or namespace selectors. You can even choose a CIDR block range to apply the network policy. In this lesson, well go through each of these options for network policies.

		Download the canal plugin:

		wget -O canal.yaml https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/canal/canal.yaml

		Apply the canal plugin:

		kubectl apply -f canal.yaml

		The YAML for a deny-all NetworkPolicy:

		apiVersion: networking.k8s.io/v1
		kind: NetworkPolicy
		metadata:
		  name: deny-all
		spec:
		  podSelector: {}
		  policyTypes:
		  - Ingress

		Run a deployment to test the NetworkPolicy:

		kubectl run nginx --image=nginx --replicas=2

		Create a service for the deployment:

		kubectl expose deployment nginx --port=80

		Attempt to access the service by using a busybox interactive pod:

		kubectl run busybox --rm -it --image=busybox /bin/sh
		#wget --spider --timeout=1 nginx

		The YAML for a pod selector NetworkPolicy:

		apiVersion: networking.k8s.io/v1
		kind: NetworkPolicy
		metadata:
		  name: db-netpolicy
		spec:
		  podSelector:
			matchLabels:
			  app: db
		  ingress:
		  - from:
			- podSelector:
				matchLabels:
				  app: web
			ports:
			- port: 5432

		Label a pod to get the NetworkPolicy:

		kubectl label pods [pod_name] app=db

		The YAML for a namespace NetworkPolicy:

		apiVersion: networking.k8s.io/v1
		kind: NetworkPolicy
		metadata:
		  name: ns-netpolicy
		spec:
		  podSelector:
			matchLabels:
			  app: db
		  ingress:
		  - from:
			- namespaceSelector:
				matchLabels:
				  tenant: web
			ports:
			- port: 5432

		The YAML for an IP block NetworkPolicy:

		apiVersion: networking.k8s.io/v1
		kind: NetworkPolicy
		metadata:
		  name: ipblock-netpolicy
		spec:
		  podSelector:
			matchLabels:
			  app: db
		  ingress:
		  - from:
			- ipBlock:
				cidr: 192.168.1.0/24

		The YAML for an egress NetworkPolicy:

		apiVersion: networking.k8s.io/v1
		kind: NetworkPolicy
		metadata:
		  name: egress-netpol
		spec:
		  podSelector:
			matchLabels:
			  app: web
		  egress:
		  - to:
			- podSelector:
				matchLabels:
				  app: db
			ports:
			- port: 5432

	TLS certificates:
		ca is used to generate a TLS cert and authenticate with a api server
		ca cert bundle is automatically placed on every pod cluster creates, at
		/var/run/secrets/kubernetes.io/serviceaccount

		root@nginx-7db9fccd9b-5qwhn:/var/run/secrets/kubernetes.io/serviceaccount# ls -ltr
		total 0
		lrwxrwxrwx 1 root root 12 Nov 25 18:17 token -> ..data/token
		lrwxrwxrwx 1 root root 16 Nov 25 18:17 namespace -> ..data/namespace
		lrwxrwxrwx 1 root root 13 Nov 25 18:17 ca.crt -> ..data/ca.crt
		root@nginx-7db9fccd9b-5qwhn:/var/run/secrets/kubernetes.io/serviceaccount#

		**ca.crt - cert authority - token of namespace/default sa of the pod.
		
		there's a built-in api within kubernetes to build and use custom certificates.
		downloading cfssl binaries to generate a csr (cert signing request)
		
		$ wget -q --show-progress --https-only --timestamping https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
		$ ls -ltr cfssl*
		-rw-rw-r-- 1 cloud_user cloud_user 10376657 Mar 30  2016 cfssl_linux-amd64
		-rw-rw-r-- 1 cloud_user cloud_user  2277873 Mar 30  2016 cfssljson_linux-amd64

		move them over to /usr/local/bin/ and rename without the _linux-adm64 part.
		$ cfssl version
		Version: 1.2.0
		Revision: dev
		Runtime: go1.6

		create a csr:
		cat <<EOF | cfssl genkey - | cfssljson -bare server
		{
		  "hosts": [
			"my-svc.my-namespace.svc.cluster.local",
			"my-pod.my-namespace.pod.cluster.local",
			"172.168.0.24",
			"10.0.34.2"
		  ],
		  "CN": "my-pod.my-namespace.pod.cluster.local",
		  "key": {
			"algo": "ecdsa",
			"size": 256
		  }
		}
		EOF
		
		$ ls -ltr server*
		-rw-r--r-- 1 cloud_user cloud_user 558 Nov 25 18:47 server.csr
		-rw------- 1 cloud_user cloud_user 227 Nov 25 18:47 server-key.pem

		now using the csr for server created, create a csr kubernetes object like so:
		$ cat csr.yaml
		apiVersion: certificates.k8s.io/v1beta1
		kind: CertificateSigningRequest
		metadata:
		  name: pod-csr.web
		spec:
		  groups:
		  - system:authenticated
		  request: $(cat server.csr | base64 | tr -d '\n')
		  usages:
		  - digital signature
		  - key encipherment
		  - server path

		## request: $() was not working for me;
		ran the command on command line and placed the stdout and ran apply, it worked.
		
		$ kubectl get csr
		NAME          AGE   REQUESTOR          CONDITION
		pod-csr.web   49s   kubernetes-admin   Pending

		the csr can be approved and issued bv the admin using certificate approve:
		$ kubectl get csr
		NAME          AGE     REQUESTOR          CONDITION
		pod-csr.web   2m38s   kubernetes-admin   Pending
		cloud_user@chaitanyah3681c:~/practice-7$ kubectl certificate approve pod-csr.web
		certificatesigningrequest.certificates.k8s.io/pod-csr.web approved
		cloud_user@chaitanyah3681c:~/practice-7$ kubectl get csr              NAME          AGE     REQUESTOR          CONDITION
		pod-csr.web   2m50s   kubernetes-admin   Approved,Issued

		u can extract the cert and decode it and save it into a file; can use the file to authenticate yourself to the api server.

---
		
		A Certificate Authority (CA) is used to generate TLS certificates and authenticate to your API server. In this lesson, well go through certificate requests and generating a new certificate.

		Find the CA certificate on a pod in your cluster:

		kubectl exec busybox -- ls /var/run/secrets/kubernetes.io/serviceaccount

		Download the binaries for the cfssl tool:

		wget -q --show-progress --https-only --timestamping \
		  https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \
		  https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64

		Make the binary files executable:

		chmod +x cfssl_linux-amd64 cfssljson_linux-amd64

		Move the files into your bin directory:

		sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl

		sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson

		Check to see if you have cfssl installed correctly:

		cfssl version

		Create a CSR file:

		cat <<EOF | cfssl genkey - | cfssljson -bare server
		{
		  "hosts": [
			"my-svc.my-namespace.svc.cluster.local",
			"my-pod.my-namespace.pod.cluster.local",
			"172.168.0.24",
			"10.0.34.2"
		  ],
		  "CN": "my-pod.my-namespace.pod.cluster.local",
		  "key": {
			"algo": "ecdsa",
			"size": 256
		  }
		}
		EOF

		Create a CertificateSigningRequest API object:

		cat <<EOF | kubectl create -f -
		apiVersion: certificates.k8s.io/v1beta1
		kind: CertificateSigningRequest
		metadata:
		  name: pod-csr.web
		spec:
		  groups:
		  - system:authenticated
		  request: $(cat server.csr | base64 | tr -d '\n')
		  usages:
		  - digital signature
		  - key encipherment
		  - server auth
		EOF

		View the CSRs in the cluster:

		kubectl get csr

		View additional details about the CSR:

		kubectl describe csr pod-csr.web

		Approve the CSR:

		kubectl certificate approve pod-csr.web

		View the certificate within your CSR:

		kubectl get csr pod-csr.web -o yaml

		Extract and decode your certificate to use in a file:

		kubectl get csr pod-csr.web -o jsonpath='{.status.certificate}' \
			| base64 --decode > server.crt


	secure images:
		config.json contains information on the docker registry to use and auth details
		# cat /home/cloud_user/.docker/config.json
		{
				"auths": {
						"https://index.docker.io/v1/": {
								"auth": "Y2hhaXRhbnlhaDM2ODpEZXZpbGV5ZXMx"
						}
				},
				"HttpHeaders": {
						"User-Agent": "Docker-Client/18.06.1-ce (linux)"
				}

		u can login to the registry and access the registry for iamges from your remote server.
		u need to always pull images using ImagePullPolicy to Always
		so as to prevent users who have access to the remote server, access to the private iamges 
		pulled from the remote private registry.
		
		u can login to ur private registry - azure, aws etc or local.
		
		kubernetes uses secrets to authenticate itself to the registry and pull images for the sake of creating pods.
		create new secret. 
		patch the default service account to include imagePullSecrets and give ur new secret.
		from then on, it will use this image secret to pull new images. 
		
		here, if secret is setup, it will show the sa image pull secret:
		
		$ kubectl describe sa default
		Name:                default
		Namespace:           default
		Labels:              <none>
		Annotations:         <none>
		Image pull secrets:  <none>
		Mountable secrets:   default-token-nlp2l
		Tokens:              default-token-nlp2l
		Events:              <none>


	security context:
		limit access to certain objects at a pod and container level.
		default pod without security contexts:
		$ kubectl run pod-with-defaults --image alpine --restart Never -- /bin/sleep 99999999
		pod/pod-with-defaults created
		cloud_user@chaitanyah3681c:~$ kubectl get pods
		NAME                     READY   STATUS    RESTARTS   AGE
		nginx-7db9fccd9b-5qwhn   1/1     Running   0          125m
		nginx-7db9fccd9b-zrgpm   1/1     Running   0          125m
		pod-with-defaults        1/1     Running   0          3s

		pod running as root here:
		$ kubectl exec -it pod-with-defaults id
		uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video)

		this is based on the dockerfile for the image alpine.
		
		adding security context to run as a guest user instead:
		$ cat alpine-user-context.yaml apiVersion: v1
		kind: Pod
		metadata:
		  name: alpine-user-context
		spec:
		  containers:
		  - name: pod1
			image: alpine
			command: ["/bin/sleep","999999"]
			securityContext:
			  runAsUser: 405 # guest user
		cloud_user@chaitanyah3681c:~/practice-7$ kubectl get pods
		NAME                  READY   STATUS    RESTARTS   AGE
		alpine-user-context   1/1     Running   0          12s
		pod-with-defaults     1/1     Running   0          4m6s
		cloud_user@chaitanyah3681c:~/practice-7$ kubectl exec pod-with-defaults id
		uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video)
		cloud_user@chaitanyah3681c:~/practice-7$ kubectl exec alpine-user-context id
		uid=405(guest) gid=100(users)

		to not run as root:
		securityContext:
		  runAsNonRoot: true
		  
		notice the pod can fail here as if the dockerfile wants the container to run as root.
		
		Events:
		  Type     Reason     Age                From                                      Message
		  ----     ------     ----               ----                                      -------
		  Normal   Scheduled  27s                default-scheduler                         Successfully assigned default/alpine-runasnonroot to chaitanyah3682c.mylabserver.com
		  Normal   Pulling    10s (x3 over 27s)  kubelet, chaitanyah3682c.mylabserver.com  Pulling image "alpine"
		  Normal   Pulled     10s (x3 over 26s)  kubelet, chaitanyah3682c.mylabserver.com  Successfully pulled image "alpine"
		  Warning  Failed     10s (x3 over 26s)  kubelet, chaitanyah3682c.mylabserver.com  Error: container has runAsNonRoot and image will run as root

		
		privileged mode:
		securityContext:
		  privileged: true
		  
		to be able to make changes at a kernel level on the pods:
			u cant normally modify the time on a pod or a container.
			u can add a capability to modify SYS_TIME which allows u to then modify the time
			of a pod by issuing a date command:
			securityContext:
			  capabilities:
			    add:
				- SYS_TIME
			NET_ADMIN to add network admin privileges.
			
		to remove capabilities:
			securityContext:
			  capabilities:
			    drop:
				-  CHOWN
		
			wont allow u to change ownership of filesystems on the pod.
			
		if u want a pod to not be able to write anything to the / filesystem
		but only use /volume for its volume storage:
		$ cat readonly-pod.yaml
		apiVersion: v1
		kind: Pod
		metadata:
		  name: readonly-pod
		spec:
		  containers:
		  - name: pod1
			image: alpine
			command: ["/bin/sleep","99999999"]
			securityContext:
			  readOnlyRootFilesystem: true
			volumeMounts:
			- name: vol1
			  mountPath: /volume
			  readOnly: false
		  volumes:
		  - name: vol1
			emptyDir: {}
		cloud_user@chaitanyah3681c:~/practice-7$ kubectl get pods
		NAME                  READY   STATUS    RESTARTS   AGE
		alpine-user-context   1/1     Running   0          11m
		pod-with-defaults     1/1     Running   0          14m
		readonly-pod          1/1     Running   0          10s

		all the above were at a container level.
		
		u can do at pod level too:
		 under spec:
		  securityContext:
		    fsGroup: 555 # first user gid.
			supplementalGroups: [666,777] # supplemental groups.


---

		Defining security contexts allows you to lock down your containers, so that only certain processes can do certain things. This ensures the stability of your containers and allows you to give control or take it away. In this lesson, well go through how to set the security context at the container level and the pod level.

		Run an alpine container with default security:

		kubectl run pod-with-defaults --image alpine --restart Never -- /bin/sleep 999999

		Check the ID on the container:

		kubectl exec pod-with-defaults id

		The YAML for a container that runs as a user:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: alpine-user-context
		spec:
		  containers:
		  - name: main
			image: alpine
			command: ["/bin/sleep", "999999"]
			securityContext:
			  runAsUser: 405

		Create a pod that runs the container as user:

		kubectl apply -f alpine-user-context.yaml

		View the IDs of the new pod created with container user permission:

		kubectl exec alpine-user-context id

		The YAML for a pod that runs the container as non-root:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: alpine-nonroot
		spec:
		  containers:
		  - name: main
			image: alpine
			command: ["/bin/sleep", "999999"]
			securityContext:
			  runAsNonRoot: true

		Create a pod that runs the container as non-root:

		kubectl apply -f alpine-nonroot.yaml

		View more information about the pod error:

		kubectl describe pod alpine-nonroot

		The YAML for a privileged container pod:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: privileged-pod
		spec:
		  containers:
		  - name: main
			image: alpine
			command: ["/bin/sleep", "999999"]
			securityContext:
			  privileged: true

		Create the privileged container pod:

		kubectl apply -f privileged-pod.yaml

		View the devices on the default container:

		kubectl exec -it pod-with-defaults ls /dev

		View the devices on the privileged pod container:

		kubectl exec -it privileged-pod ls /dev

		Try to change the time on a default container pod:

		kubectl exec -it pod-with-defaults -- date +%T -s "12:00:00"

		The YAML for a container that will allow you to change the time:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: kernelchange-pod
		spec:
		  containers:
		  - name: main
			image: alpine
			command: ["/bin/sleep", "999999"]
			securityContext:
			  capabilities:
				add:
				- SYS_TIME

		Create the pod that will allow you to change the containers time:

		kubectl run -f kernelchange-pod.yaml

		Change the time on a container:

		kubectl exec -it kernelchange-pod -- date +%T -s "12:00:00"

		View the date on the container:

		kubectl exec -it kernelchange-pod -- date

		The YAML for a container that removes capabilities:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: remove-capabilities
		spec:
		  containers:
		  - name: main
			image: alpine
			command: ["/bin/sleep", "999999"]
			securityContext:
			  capabilities:
				drop:
				- CHOWN

		Create a pod thats container has capabilities removed:

		kubectl apply -f remove-capabilities.yaml

		Try to change the ownership of a container with removed capability:

		kubectl exec remove-capabilities chown guest /tmp

		The YAML for a pod container that cant write to the local filesystem:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: readonly-pod
		spec:
		  containers:
		  - name: main
			image: alpine
			command: ["/bin/sleep", "999999"]
			securityContext:
			  readOnlyRootFilesystem: true
			volumeMounts:
			- name: my-volume
			  mountPath: /volume
			  readOnly: false
		  volumes:
		  - name: my-volume
			emptyDir:

		Create a pod that will not allow you to write to the local container filesystem:

		kubectl apply -f readonly-pod.yaml

		Try to write to the container filesystem:

		kubectl exec -it readonly-pod touch /new-file

		Create a file on the volume mounted to the container:

		kubectl exec -it readonly-pod touch /volume/newfile

		View the file on the volume thats mounted:

		kubectl exec -it readonly-pod -- ls -la /volume/newfile

		The YAML for a pod that has different group permissions for different pods:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: group-context
		spec:
		  securityContext:
			fsGroup: 555
			supplementalGroups: [666, 777]
		  containers:
		  - name: first
			image: alpine
			command: ["/bin/sleep", "999999"]
			securityContext:
			  runAsUser: 1111
			volumeMounts:
			- name: shared-volume
			  mountPath: /volume
			  readOnly: false
		  - name: second
			image: alpine
			command: ["/bin/sleep", "999999"]
			securityContext:
			  runAsUser: 2222
			volumeMounts:
			- name: shared-volume
			  mountPath: /volume
			  readOnly: false
		  volumes:
		  - name: shared-volume
			emptyDir:

		Create a pod with two containers and different group permissions:

		kubectl apply -f group-context.yaml

		Open a shell to the first container on that pod:

		kubectl exec -it group-context -c first sh

	securing persistent key value store:
		persistent storage that outlives the lifetime of a pod
		u can create secrets and mount them as data to a pod.
		expose secrets as volumes in a pod
		not best practice to expose secrets to env variables 
		tmpfs is used for secrets - in memory storage
		
		sa has a secret that has data.
		default sa has a secret which has ca bundle info thats placed on
		every pod in /var/run/secrets/kubernetes.io/serviceaccount/ dir.
		
		ca bundle -> ca.crt, namespace and token file.
		
		example showing how to create a secret and have it be shared to your pod as a temporary file system tmpfs
		generate a private rsa key:
		$ openssl genrsa -out https.key 2048
		
		generate a cert for your server:
		$ openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN=www.example.com
		
		we created a key and a cert and then a dummy file to create a secret out of this three.
		-rw------- 1 cloud_user cloud_user 1679 Nov 26 18:32 https.key
		-rw-rw-r-- 1 cloud_user cloud_user 1131 Nov 26 18:33 https.cert
		-rw-rw-r-- 1 cloud_user cloud_user   22 Nov 26 18:39 file

		$ kubectl create secret generic example-https --from-file=https.key --from-file=https.cert --from-file=file
		secret/example-https created

		$ kubectl get secrets
		NAME                  TYPE                                  DATA   AGE
		appsecret             Opaque                                2      200d
		default-token-nlp2l   kubernetes.io/service-account-token   3      205d
		example-https         Opaque                                3      28s
		jenkins-token-dxsvf   kubernetes.io/service-account-token   3      166d

		$ kubectl get secret example-https -o yaml
		apiVersion: v1
		data:
		  file: dGhpcyBpcyBkdW1teSBjb250ZW50Cg==
		  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURGVENDQWYyZ0F3SUJBZ0lVRWp0d0haWEg1R3JHakZJSGtyamc3RVBSclk4d0RRWUpLb1pJaHZjTkFRRUwKQlFBd0dqRVlNQllHQTFVRUF3d1BkM2QzTG1WNFlXMXdiR1V1WTI5dE1CNFhEVEU1TVRFeU5qRTRNek0xT0ZvWApEVEk1TVRFeU16RTRNek0xT0Zvd0dqRVlNQllHQTFVRUF3d1BkM2QzTG1WNFlXMXdiR1V1WTI5dE1JSUJJakFOCkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQTAxdGd0TXI2azRGRUtkNVYzMEEzanB2bnlMbFcKNk9GWE03S041REtXa2IxYkE4cC9RamozeUpCdldUSnF2dlc5K0ZEL0o2dHh1QS9icnNraDhFUTZHeHpiUzNsWQowL1FibTZ3ZzFTdU80SzI0RnBsb1hCK1NubDNNbFh6RktHZTJyQ1QzWHFhRktFMTliTWRaZG5qMENZejFJNldMCnY5b2JKejE1Qm45UktycmFRR3BIVkxXeHdNSHJnejhLTmloVFo5aEREd1FOODl3MGxYUHF6ZXZJdVVtUHRhRW4KWGxsV2haYXZBbzZmbHdqbkk3SGFiVXZIOGcrbjFjdS9HT0o1K3dDVVNPL2hub2JNZ0RWMSs5N3VlTDNIWk9uWAorREEzalVVa0lzeVZ4ZFRHV2w4WkkvM3JQYTZDbjVybkZ2dVBma3c2OXdBWDdlSi81c1JGemg0TjBRSURBUUFCCm8xTXdVVEFkQmdOVkhRNEVGZ1FVNG9WMnJidFVCY1E0OHJRSnZrMHA1UHZQdzZBd0h3WURWUjBqQkJnd0ZvQVUKNG9WMnJidFVCY1E0OHJRSnZrMHA1UHZQdzZBd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBTkJna3Foa2lHOXcwQgpBUXNGQUFPQ0FRRUFaVDhXQS9VM0MzVzdtYkZEdTh4VTRuVTQvc1lLbnhzdFZFNFZhOTI1MW9hOWVkZFBZQkJkCnZUY0t3OXg2OHRNUE10bW5YckovUFM5ajU0YmdwK3JwaTVlOVc5VEFydEh6VGNLbWF4dWFia1ZuSFI2T2pRa3UKZ05KN2I0UFplcjBIeFp4REdLR2Y5UEhGb1h1Z3J5WGpHKy9lTlNBaXhmb3A1KzF3VERjdDFXaXRYTVVZWTlxLwo5M3Y4WWhOV3lTUVh2WmdJNURnSHlhMTNSeGRqS2Yrd1NKbkV6STdwS1VqKzlwV21EOTJKYkU0UEx5bzNhc0krCmVKVjBpY25FL1hndE92d01neXpmSmRGczRlSSs3M3BkSjlWdmNZTjREZmhqMVlNakJQeENrakRqdEpuc3NlcGwKbW56czRzcUlxRXFPTDRVc29nYmU2dUtJMnVsbFlHZHZNQT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
		  https.key: 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
		kind: Secret
		metadata:
		  creationTimestamp: "2019-11-26T18:41:04Z"
		  name: example-https
		  namespace: default
		  resourceVersion: "2195928"
		  selfLink: /api/v1/namespaces/default/secrets/example-https
		  uid: 4d518f92-107c-11ea-821b-0ae96c07b466
		type: Opaque
		cloud_user@chaitanyah3681c:/tmp$

		notice here, that while creating the secret, kubernetes encodes the secrets and on applying it to the pod
		decodes it, so the app running on the pod doesnt have to do so.
		
		now writing a yaml for a pod that can use this secret.
		
			created a configmap to go wit the pod:

			$ cat secret-using-pod-configmap.yaml
			apiVersion: v1
			kind: ConfigMap
			metadata:
			  name: config
			data:
			  my-nginx-config.conf: |
				server {
				  listen                    80;
				  listen                    443 ssl;
				  server_name               www.example.com;
				  ssl_certificate           certs/https.cert;
				  ssl_certificate_key       certs/https.key;
				  ssl_protocols             TLSv1 TLSv1.1 TLSv1.2;
				  ssl_ciphers               HIGH:!aNULL:!MDS;

				  location / {
					root    /usr/share/nginx/html;
					index   index.html index.htm;
					}

				  }
			  sleep-interval: |
					25

			**u need the | in the value of each key; it fails otherwise.
			
			$ kubectl get configmap config -o yaml
			apiVersion: v1
			data:
			  my-nginx-config.conf: "server {\n  listen                    80;\n  listen                    443
				ssl;\n  server_name               www.example.com;\n  ssl_certificate           certs/https.cert;\n
				\ ssl_certificate_key       certs/https.key;\n  ssl_protocols             TLSv1
				TLSv1.1 TLSv1.2;\n  ssl_ciphers               HIGH:!aNULL:!MDS;\n\n  location
				/ {\n    root    /usr/share/nginx/html;\n    index   index.html index.htm;        \n
				\   }\n\n  }\n"
			  sleep-interval: |
				25
			kind: ConfigMap
			metadata:
			  annotations:
				kubectl.kubernetes.io/last-applied-configuration: |
				  {"apiVersion":"v1","data":{"my-nginx-config.conf":"server {\n  listen                    80;\n  listen                    443 ssl;\n  server_name               www.example.com;\n  ssl_certificate           certs/https.cert;\n  ssl_certificate_key       certs/https.key;\n  ssl_protocols             TLSv1 TLSv1.1 TLSv1.2;\n  ssl_ciphers               HIGH:!aNULL:!MDS;\n\n  location / {\n    root    /usr/share/nginx/html;\n    index   index.html index.htm;        \n    }\n\n  }\n","sleep-interval":"25\n"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"config","namespace":"default"}}
			  creationTimestamp: "2019-11-26T18:57:45Z"
			  name: config
			  namespace: default
			  resourceVersion: "2197629"
			  selfLink: /api/v1/namespaces/default/configmaps/config
			  uid: a1a9c158-107e-11ea-821b-0ae96c07b466

			$ cat secret-using-pod.yaml
			apiVersion: v1
			kind: Pod
			metadata:
			  name: secret-using-pod
			spec:
			  containers:
			  - name: html-web
				image: linuxacademycontent/fortune
				env:
				- name: INTERVAL
				  valueFrom:
					configMapKeyRef:
					  name: config
					  key: sleep-interval
				volumeMounts:
				- name: html
				  mountPath: /var/htdocs
			  - name: web-server
				image: nginx:alpine
				volumeMounts:
				- name: html
				  mountPath: /usr/share/nginx/html
				  readOnly: true
				- name: config
				  mountPath: /etc/nginx/conf.d
				  readOnly: true
				- name: certs
				  mountPath: /etc/nginx/certs/
				  readOnly: true
				ports:
				- containerPort: 80
				- containerPort: 443
			  volumes:
			  - name: html
				emptyDir: {}
			  - name: config
				configMap:
				  name: config
				  items:
				  - key: my-nginx-config.conf
					path: https.conf
			  - name: certs
				secret:
				  secretName: example-https

			created the pod with the config map:
			both pods have hostname of the name of the pod as expected.
			first container has env var INTERVAL set to value in configMap.
			has /var/htdocs mounted.

			second pod has /usr/share/nginx/html and /etc/config/conf.d mounted as expected and has secret certs as tmpfs.
			# mount | grep -E 'certs|config|html'
			tmpfs on /etc/nginx/certs type tmpfs (ro,relatime)
			/dev/xvda1 on /usr/share/nginx/html type ext4 (ro,relatime,discard,data=ordered)
			
			config doesnt show up here as it not a volume thing; obtained using configMaps instead.
			
			$ kc create cm pup-config --from-file=./puppet.conf
			configmap/pup-config created
			cloud_user@chaitanyah3681c:~$ kubectl get cm
			NAME         DATA   AGE
			pup-config   1      3s
			cloud_user@chaitanyah3681c:~$ kubectl get cm -o yaml
			apiVersion: v1
			items:
			- apiVersion: v1
			  data:
				puppet.conf: |
				  [master]
				  server=pup1
				  certname=pup1

				  [agent]
				  server=pup1
				  certname=pup1
			  kind: ConfigMap
			  metadata:
				creationTimestamp: "2020-01-02T21:19:49Z"
				name: pup-config
				namespace: default
				resourceVersion: "91257"
				selfLink: /api/v1/namespaces/default/configmaps/pup-config
				uid: 166eee37-09e2-48b9-b36a-b4797553c3b3
			kind: List
			metadata:
			  resourceVersion: ""
			  selfLink: ""



****logging and monitoring:

	monitoring cluster components:
	
		We are able to monitor the CPU and memory utilization of our pods and nodes by using the metrics server. In this lesson, well install the metrics server and see how the kubectl top command works.

		Clone the metrics server repository:

		git clone https://github.com/linuxacademy/metrics-server

		Install the metrics server in your cluster:

		kubectl apply -f ~/metrics-server/deploy/1.8+/

		Get a response from the metrics server API:

		kubectl get --raw /apis/metrics.k8s.io/

		Get the CPU and memory utilization of the nodes in your cluster:

		kubectl top node

		Get the CPU and memory utilization of the pods in your cluster:

		kubectl top pods

		Get the CPU and memory of pods in all namespaces:

		kubectl top pods --all-namespaces

		Get the CPU and memory of pods in only one namespace:

		kubectl top pods -n kube-system

		Get the CPU and memory of pods with a label selector:

		kubectl top pod -l run=pod-with-defaults

		Get the CPU and memory of a specific pod:

		kubectl top pod pod-with-defaults

		Get the CPU and memory of the containers inside the pod:

		kubectl top pods group-context --containers

	
	monitoring the apps running within a cluster:
	
		There are ways Kubernetes can automatically monitor your apps for you and, furthermore, fix them by either restarting or preventing them from affecting the rest of your service. You can insert liveness probes and readiness probes to do just this for custom monitoring of your applications.

		The pod YAML for a liveness probe:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: liveness
		spec:
		  containers:
		  - image: linuxacademycontent/kubeserve
			name: kubeserve
			livenessProbe:
			  httpGet:
				path: /
				port: 80

		The YAML for a service and two pods with readiness probes:

		apiVersion: v1
		kind: Service
		metadata:
		  name: nginx
		spec:
		  type: LoadBalancer
		  ports:
		  - port: 80
			targetPort: 80
		  selector:
			app: nginx
		---
		apiVersion: v1
		kind: Pod
		metadata:
		  name: nginx
		  labels:
			app: nginx
		spec:
		  containers:
		  - name: nginx
			image: nginx
			readinessProbe:
			  httpGet:
				path: /
				port: 80
			  initialDelaySeconds: 5
			  periodSeconds: 5
		---
		apiVersion: v1
		kind: Pod
		metadata:
		  name: nginxpd
		  labels:
			app: nginx
		spec:
		  containers:
		  - name: nginx
			image: nginx:191
			readinessProbe:
			  httpGet:
				path: /
				port: 80
			  initialDelaySeconds: 5
			  periodSeconds: 5

		Create the service and two pods with readiness probes:

		kubectl apply -f readiness.yaml

		Check if the readiness check passed or failed:

		kubectl get pods

		Check if the failed pod has been added to the list of endpoints:

		kubectl get ep

		Edit the pod to fix the problem and enter it back into the service:

		kubectl edit pod [pod_name]

		Get the list of endpoints to see that the repaired pod is part of the service again:

		kubectl get ep


	managing cluster component logs:
	
		There are many ways to manage the logs that can accumulate from both applications and system components. In this lesson, well go through a few different approaches to organizing your logs.

		The directory where the continainer logs reside:

		/var/log/containers

		The directory where kubelet stores its logs:

		/var/log

		The YAML for a pod that has two different log streams:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: counter
		spec:
		  containers:
		  - name: count
			image: busybox
			args:
			- /bin/sh
			- -c
			- >
			  i=0;
			  while true;
			  do
				echo "$i: $(date)" >> /var/log/1.log;
				echo "$(date) INFO $i" >> /var/log/2.log;
				i=$((i+1));
				sleep 1;
			  done
			volumeMounts:
			- name: varlog
			  mountPath: /var/log
		  volumes:
		  - name: varlog
			emptyDir: {}

		Create a pod that has two different log streams to the same directory:

		kubectl apply -f twolog.yaml

		View the logs in the /var/log directory of the container:

		kubectl exec counter -- ls /var/log

		The YAML for a sidecar container that will tail the logs for each type:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: counter
		spec:
		  containers:
		  - name: count
			image: busybox
			args:
			- /bin/sh
			- -c
			- >
			  i=0;
			  while true;
			  do
				echo "$i: $(date)" >> /var/log/1.log;
				echo "$(date) INFO $i" >> /var/log/2.log;
				i=$((i+1));
				sleep 1;
			  done
			volumeMounts:
			- name: varlog
			  mountPath: /var/log
		  - name: count-log-1
			image: busybox
			args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log']
			volumeMounts:
			- name: varlog
			  mountPat\
  			  /var/log
		  - name: count-log-2
			image: busybox
			args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log']
			volumeMounts:
			- name: varlog
			  mountPath: /var/log
		  volumes:
		  - name: varlog
			emptyDir: {}

		View the first type of logs separately:

		kubectl logs counter count-log-1

		View the second type of logs separately:

		kubectl logs counter count-log-2


	managing app logs:

		Containerized applications usually write their logs to standard out and standard error instead of writing their logs to files. Docker then redirects those streams to files. You can retrieve those files with the kubectl logs command in Kubernetes. In this lesson, well go over the many ways to manipulate the output of your logs and redirect them to a file.

		Get the logs from a pod:

		kubectl logs nginx

		Get the logs from a specific container on a pod:

		kubectl logs counter -c count-log-1

		Get the logs from all containers on the pod:

		kubectl logs counter --all-containers=true

		Get the logs from containers with a certain label:

		kubectl logs -lapp=nginx

		Get the logs from a previously terminated container within a pod:

		kubectl logs -p -c nginx nginx

		Stream the logs from a container in a pod:

		kubectl logs -f -c count-log-1 counter

		Tail the logs to only view a certain number of lines:

		kubectl logs --tail=20 nginx

		View the logs from a previous time duration:

		kubectl logs --since=1h nginx

		View the logs from a container within a pod within a deployment:

		kubectl logs deployment/nginx -c nginx

		Redirect the output of the logs to a file:

		kubectl logs counter -c count-log-1 > count.log


troubleshooting:

	troubleshooting app failure:
	
		Application failure can happen for many reasons, but there are ways within Kubernetes that make it a little easier to discover why. In this lesson, well fix some broken pods and show common methods to troubleshoot.

		The YAML for a pod with a termination reason:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: pod2
		spec:
		  containers:
		  - image: busybox
			name: main
			command:
			- sh
			- -c
			- 'echo "I''ve had enough" > /var/termination-reason ; exit 1'
			terminationMessagePath: /var/termination-reason

		One of the first steps in troubleshooting is usually to describe the pod:

		kubectl describe po pod2

		The YAML for a liveness probe that checks for pod health:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: liveness
		spec:
		  containers:
		  - image: linuxacademycontent/kubeserve
			name: kubeserve
			livenessProbe:
			  httpGet:
				path: /healthz
				port: 8081

		View the logs for additional detail:

		kubectl logs pod-with-defaults

		Export the YAML of a running pod, in the case that you are unable to edit it directly:

		kubectl get po pod-with-defaults -o yaml --export > defaults-pod.yaml

		Edit a pod directly (i.e., changing the image):

		kubectl edit po nginx


	troubleshooting control plane failure:
			
		The Kubernetes Control Plane is an important component to back up and protect against failure. There are certain best practices you can take to ensure you dont have a single point of failure. If your Control Plane components are not effectively communicating, there are a few things you can check to ensure your cluster is operating efficiently.

		Check the events in the kube-system namespace for errors:

		kubectl get events -n kube-system

		Get the logs from the individual pods in your kube-system namespace and check for errors:

		kubectl logs [kube_scheduler_pod_name] -n kube-system

		Check the status of the Docker service:

		sudo systemctl status docker

		Start up and enable the Docker service, so it starts upon bootup:

		sudo systemctl enable docker && systemctl start docker

		Check the status of the kubelet service:

		sudo systemctl status kubelet

		Start up and enable the kubelet service, so it starts up when the machine is rebooted:

		sudo systemctl enable kubelet && systemctl start kubelet

		Turn off swap on your machine:

		sudo su -
		swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab

		Check if you have a firewall running:

		sudo systemctl status firewalld

		Disable the firewall and stop the firewalld service:

		sudo systemctl disable firewalld && systemctl stop firewalld

		
	troubleshooting worker node failure:
			
		kubectl get nodes

		Find out more information about the nodes with kubectl describe:

		kubectl describe nodes chadcrowell2c.mylabserver.com

		You can try to log in to your server via SSH:

		ssh chadcrowell2c.mylabserver.com

		Get the IP address of your nodes:

		kubectl get nodes -o wide

		Use the IP address to further probe the server:

		ssh cloud_user@172.31.29.182

		Generate a new token after spinning up a new server:

		sudo kubeadm token generate

		Create the kubeadm join command for your new worker node:

		sudo kubeadm token create [token_name] --ttl 2h --print-join-command

		View the journalctl logs:

		sudo journalctl -u kubelet

		View the syslogs:

		sudo more syslog | tail -120 | grep kubelet


	troubleshooting networking:
	
		Run a deployment using the container port 9376 and with three replicas:

		kubectl run hostnames --image=k8s.gcr.io/serve_hostname \
								--labels=app=hostnames \
								--port=9376 \
								--replicas=3

		List the services in your cluster:

		kubectl get svc

		Create a service by exposing a port on the deployment:

		kubectl expose deployment hostnames --port=80 --target-port=9376

		Run an interactive busybox pod:

		kubectl run -it --rm --restart=Never busybox --image=busybox:1.28 sh

		From the pod, check if DNS is resolving hostnames:

		# nslookup hostnames

		From the pod, cat out the /etc/resolv.conf file:

		# cat /etc/resolv.conf

		From the pod, look up the DNS name of the Kubernetes service:

		# nslookup kubernetes.default

		Get the JSON output of your service:

		kubectl get svc hostnames -o json

		View the endpoints for your service:

		kubectl get ep

		Communicate with the pod directly (without the service):

		wget -qO- 10.244.1.6:9376

		Check if kube-proxy is running on the nodes:

		ps auxw | grep kube-proxy

		Check if kube-proxy is writing iptables:

		iptables-save | grep hostnames

		View the list of kube-system pods:

		kubectl get pods -n kube-system

		Connect to your kube-proxy pod in the kube-system namespace:

		kubectl exec -it kube-proxy-cqptg -n kube-system -- sh

		Delete the flannel CNI plugin:

		kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml

		Apply the Weave Net CNI plugin:

		kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"




CKA practice exam part 1:

You have been given access to a three-node cluster. Within that cluster, you will be responsible for creating a service for end users of your web application. You will ensure the application meets the specifications set by the developers and the proper resources are in place to ensure maximum uptime for the app. You must perform the following tasks in order to complete this hands-on lab:

    All objects should be in the web namespace.
    The deployment name should be webapp.
    The deployment should have 3 replicas.
    The deployments pods should have one container using the linuxacademycontent/podofminerva image with the tag latest.
    The service should be named web-service.
    The service should forward traffic to port 80 on the pods.
    The service should be exposed externally by listening on port 30080 on each node.
    The pods should be configured to check the /healthz. endpoint on port 8081, and automatically restart the container if the check fails.
    The pods should be configured to not receive traffic until the endpoint on port 80 responds successfully.



$ cat lka-practice-exam-1.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  namespace: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: cont
        image: linuxacademycontent/podofminerva
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
        readinessProbe:
          httpGet:
            path: /
            port: 80

---

apiVersion: v1
kind: Service
metadata:
  name: web-service
  namespace: web
spec:
  selector:
    app: webapp
  type: NodePort
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30080

	
$ kubectl get deployments -n web
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
webapp   3/3     1            3           4m9s
$ kubectl get pods -n web
NAME                      READY   STATUS    RESTARTS   AGE
webapp-845cb5d759-qlpp7   0/1     Running   0          3m26s
webapp-8469474d88-ff6lr   1/1     Running   0          4m19s
webapp-8469474d88-hnlb8   1/1     Running   0          4m19s
webapp-8469474d88-wlbj4   1/1     Running   0          4m19s

-----------

cka exam part 2:

You have been given access to a two-node cluster. Within that cluster, a PersistentVolume has already been created. You must identify the size of the volume in order to make a PersistentVolumeClaim and mount the volume to your pod. Once you have created the PVC and mounted it to your running pod, you must copy the contents of /etc/passwd to the volume. Finally, you will delete the pod and create a new pod with the volume mounted in order to demonstrate the persistence of data. You must perform the following tasks in order to complete this hands-on lab:

    All objects should be in the web namespace.
    The PersistentVolumeClaim name should be data-pvc.
    The PVC request should be 256 MiB.
    The access mode for the PVC should be ReadWriteOnce.
    The storage class name should be local-storage.
    The pod name should be data-pod.
    The pod image should be busybox and the tag should be 1.28.
    The pod should request the PersistentVolumeClaim named data-pvc, and the volume name should be temp-data.
    The pod should mount the volume named temp-data to the /tmp/data directory.
    The name of the second pod should be data-pod2.


	u can have more than one pod using one pvc to bind itself to one pv.
	i created a new pod without deleting the previous one; and the file i copied over to /tmp/dataon the first pod didnt appear on the second one. 
	i deleted both; recreated the second one and now i see it. 
	
$ cat exam.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-pvc
  namespace: web
spec:
  storageClassName: local-storage
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 256Mi

---

apiVersion: v1
kind: Pod
metadata:
  name: data-pod
  namespace: web
spec:
  containers:
  - name: kk
    image: busybox:1.28
    args:
    - sleep
    - "9999"
    volumeMounts:
    - name: temp-data
      mountPath: /tmp/data
  volumes:
  - name: temp-data
    persistentVolumeClaim:
      claimName: data-pvc

---

apiVersion: v1
kind: Pod
metadata:
  name: data-pod-2
  namespace: web
spec:
  containers:
  - name: kk
    image: busybox:1.28
    args:
    - sleep
    - "9999"
    volumeMounts:
    - name: temp-data
      mountPath: /tmp/data
  volumes:
  - name: temp-data
    persistentVolumeClaim:
      claimName: data-pvc

	  


$ kubectl apply -f exam.yaml
persistentvolumeclaim/data-pvc unchanged
pod/data-pod created
pod/data-pod-2 created

$ kubectl get pods -n web
NAME         READY   STATUS    RESTARTS   AGE
data-pod     1/1     Running   0          6s
data-pod-2   1/1     Running   0          6s


$ kubectl exec -it data-pod -n web -- cp /etc/passwd /tmp/data
$ kubectl exec -it data-pod -n web -- ls -ltr /tmp/data
total 4
-rw-r--r--    1 root     root           340 Jun 25 02:36 passwd


$ kubectl exec -it data-pod-2 -n web -- cat /tmp/data/passwd
root:x:0:0:root:/root:/bin/sh
daemon:x:1:1:daemon:/usr/sbin:/bin/false
bin:x:2:2:bin:/bin:/bin/false
sys:x:3:3:sys:/dev:/bin/false
sync:x:4:100:sync:/bin:/bin/sync
mail:x:8:8:mail:/var/spool/mail:/bin/false
www-data:x:33:33:www-data:/var/www:/bin/false
operator:x:37:37:Operator:/var:/bin/false
nobody:x:65534:65534:nobody:/home:/bin/false


------------------


cka exam part 3:


You have been given access to a three-node cluster. You will be responsible for creating a deployment and a service to serve as a front end for a web application. In addition to the web application, you must deploy a Redis database and make sure the web application can only access this database using the default port of 6379. You will first create a default-deny network policy, so all pods within your Kubernetes are not able to communicate with each other by default. Then you will create a second network policy that specifies the communication on port 6379 between the web application and the database using their label selectors. You must apply these specifications to your resources in order to complete this hands-on lab:

    Create a deployment named webfront-deploy.
    The deployment should use the image nginx with the tag 1.7.8.
    The deployment should expose container port 80 on each pod and contain 2 replicas.
    Create a service named webfront-service and expose port 80, target port 80.
    The service should be exposed externally by listening on port 30080 on each node.
    Create one pod named db-redis using the image redis and the tag latest.
    Verify that you can communicate to pods by default.
    Create a network policy named default-deny that will deny pod communication by default.
    Verify that you can no longer communicate between pods.
    Apply the label role=frontend to the web application pods and the label role=db to the database pod.
    Create a network policy that will apply an ingress rule for the pods labeled with role=db to allow traffic on port 6379 from the pods labeled role=frontend.
    Verify that you have applied the correct labels and created the correct network policies.


$ cat exam.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webfront-deploy
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: k
        image: nginx:1.7.8
        ports:
        - containerPort: 80


---

apiVersion: v1
kind: Service
metadata:
  name: webfront-service
spec:
  selector:
    app: nginx
  type: NodePort
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30080

---

apiVersion: v1
kind: Pod
metadata:
  name: db-redis
spec:
  containers:
  - name: kkk
    image: redis

---

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress

---

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: redis-policy
spec:
  podSelector:
    matchLabels:
      role: db
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - port: 6379

----------------------


To clean up a stuck in terminated status namespace:
https://github.com/kubernetes/kubernetes/issues/77086 

PUT the namespace without finalizers to the /finalize subresource (kubectl proxy just lets you easily use curl against the subresource):

kubectl proxy &
PID=$!
curl -X PUT http://localhost:8001/api/v1/namespaces/delete-me/finalize -H "Content-Type: application/json" --data-binary @ns-without-finalizers.json
kill $PID


--

Using set commands to modify objects before creation

There are some object fields that dont have a flag you can use in a create command. In some of those cases, you can use a combination of set and create to specify a value for the field before object creation. This is done by piping the output of the create command to the set command, and then back to the create command. Heres an example:

kubectl create service clusterip my-svc --clusterip="None" -o yaml --dry-run | kubectl set selector --local -f - 'environment=qa' -o yaml | kubectl create -f -

    The kubectl create service -o yaml --dry-run command creates the configuration for the Service, but prints it to stdout as YAML instead of sending it to the Kubernetes API server.
    The kubectl set selector --local -f - -o yaml command reads the configuration from stdin, and writes the updated configuration to stdout as YAML.
    The kubectl create -f - command creates the object using the configuration provided via stdin.
	
--

Using --edit to modify objects before creation

You can use kubectl create --edit to make arbitrary changes to an object before it is created. Heres an example:

kubectl create service clusterip my-svc --clusterip="None" -o yaml --dry-run > /tmp/srv.yaml
kubectl create --edit -f /tmp/srv.yaml

    The kubectl create service command creates the configuration for the Service and saves it to /tmp/srv.yaml.
    The kubectl create --edit command opens the configuration file for editing before it creates the object
	

--

Migrating from imperative commands to imperative object configuration

Migrating from imperative commands to imperative object configuration involves several manual steps.

    Export the live object to a local object configuration file:

    kubectl get <kind>/<name> -o yaml > <kind>_<name>.yaml

    Manually remove the status field from the object configuration file.

    For subsequent object management, use replace exclusively.

    kubectl replace -f <kind>_<name>.yaml
	
--

Alternative: kubectl apply -f <directory/> --prune -l your=label

Only use this if you know what you are doing.

    Warning: kubectl apply --prune is in alpha, and backwards incompatible changes might be introduced in subsequent releases.

    Warning: You must be careful when using this command, so that you do not delete objects unintentionally.

As an alternative to kubectl delete, you can use kubectl apply to identify objects to be deleted after their configuration files have been removed from the directory. Apply with --prune queries the API server for all objects matching a set of labels, and attempts to match the returned live object configurations against the object configuration files. If an object matches the query, and it does not have a configuration file in the directory, and it has a last-applied-configuration annotation, it is deleted.

kubectl apply -f <directory/> --prune -l <labels>

==================================================================================

To see which Kubernetes resources are and arent in a namespace:

# In a namespace
kubectl api-resources --namespaced=true

# Not in a namespace
kubectl api-resources --namespaced=false


*8configmap colmes are read only; u can disable this by disbling the feature gate: 
/files . If you really want configMap or secrets to be rw then you can disable the feature gate - ReadOnlyAPIDataVolumes.

**KUBERNETES THE HARD WAY - https://github.com/kelseyhightower/kubernetes-the-hard-way


$ kc explain pods.spec.priorityClassName
KIND:     Pod
VERSION:  v1

FIELD:    priorityClassName <string>

DESCRIPTION:
     If specified, indicates the pod's priority. "system-node-critical" and
     "system-cluster-critical" are two special keywords which indicate the
     highest priorities with the former being the highest priority. Any other
     name must be defined by creating a PriorityClass object with that name. If
     not specified, the pod priority will be default or zero if there is no
     default.
cloud_user@chaitanyah3681c:/etc/kubernetes/manifests$ kubectl get pc
NAME                      VALUE        GLOBAL-DEFAULT   AGE
system-cluster-critical   2000000000   false            46h
system-node-critical      2000001000   false            46h
cloud_user@chaitanyah3681c:/etc/kubernetes/manifests$ kubectl get pc -o yaml
apiVersion: v1
items:
- apiVersion: scheduling.k8s.io/v1
  description: Used for system critical pods that must run in the cluster, but can
    be moved to another node if necessary.
  kind: PriorityClass
  metadata:
    creationTimestamp: "2020-01-01T00:04:58Z"
    generation: 1
    name: system-cluster-critical
    resourceVersion: "56"
    selfLink: /apis/scheduling.k8s.io/v1/priorityclasses/system-cluster-critical
    uid: 595d90d4-2c2a-11ea-832a-06bd4e919624
  value: 2000000000
- apiVersion: scheduling.k8s.io/v1
  description: Used for system critical pods that must not be moved from their current
    node.
  kind: PriorityClass
  metadata:
    creationTimestamp: "2020-01-01T00:04:58Z"
    generation: 1
    name: system-node-critical
    resourceVersion: "55"
    selfLink: /apis/scheduling.k8s.io/v1/priorityclasses/system-node-critical
    uid: 595cf55f-2c2a-11ea-832a-06bd4e919624
  value: 2000001000
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

ssh tunneling:
https://www.howtogeek.com/168145/how-to-use-ssh-tunneling/


  