introduction
	pod - smallest unit within kubernetes
		could be one container or a group of containers; 
		pods exist within the cluster formed by all the worker nodes in the kubernetes cluster, with a master.
		
	needs network, etcd and kubectl to add networking, configuration and state management to the cluster and all pods within that.

installation
	master - has kubelet, dns, network, kubectl and kubeadm
	node 1 - has kubelet, kubectl and kubeadm
	node 2 - has kubelet, kubectl and kubeadm
	
	
    The first thing that we are going to do is use SSH to log in to all machines. 
	Once we have logged in, we need to elevate privileges using sudo.
		sudo su

    Disable SELinux.
		setenforce 0 \
		&& sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux

    Enable the br_netfilter module for cluster communication.
		modprobe br_netfilter \
		&& echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables
		
    Disable swap to prevent memory allocation issues.
		 swapoff -a 
		 vim /etc/fstab.  ->  Comment out the swap line
		 **can cause the nodes to report to the master incorrect ram information because of swap. therefore, always turn it off
		 
    Install Docker CE.
		Install the Docker prerequisites.
		 yum install -y yum-utils device-mapper-persistent-data lvm2
		Add the Docker repo and install Docker.
		 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo \
		 && yum install -y docker-ce

    Add the Kubernetes repo.
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

    Install Kubernetes.
		 yum install -y kubelet kubeadm kubectl
		Reboot.

    Enable and start Docker and Kubernetes.
		systemctl enable docker \
		&& systemctl enable kubelet \
		&& systemctl start docker \
		&& systemctl start kubelet

    Check the group Docker is running in.
		docker info | grep -i cgroup

    Set Kubernetes to run in the same group.
		sed -i 's/cgroup-driver=systemd/cgroup-driver=cgroupfs/g' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

    Reload systemd for the changes to take effect, and then restart Kubernetes.
		systemctl daemon-reload \
		&& systemctl restart kubelet


	*Note: Complete the following section on the MASTER ONLY!

    Initialize the cluster using the IP range for Flannel.
		kubeadm init --pod-network-cidr=10.244.0.0/16
		Copy the kubeadmin join command.

    Exit sudo and run the following:
		mkdir -p $HOME/.kube
		sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
		sudo chown $(id -u):$(id -g) $HOME/.kube/config
		
	Deploy Flannel.
		kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
		##this is to do network overlay - cluster mode of communication mentioned in docker swarm 

    Check the cluster state.

		Kubectl get pods â€”all-namespaces

    Note: Complete the following steps on the NODES ONLY!
    Run the join command that you copied earlier, then check your nodes from the master.
	kubeadm is like a cheat command that does most of the setting up for us.

    on the master now:
	kubectl get nodes

	all needed things like on the master coredns, etcd, kube-apiserver, kube-controller-manager, 
	kube-proxy, kube-scheduler, kube-flannel get created as containers
	under kube-system namespace on the master upon following all these steps.
	on the worker nodes, u only see flannel (network overlay) and kube-proxy pods being setup
	
	
masters and nodes:
	on the master node
		etcd api-server scheduler controller-manager coredns proxy flannel- all being pods
	on the worker node
		proxy 
		flannel (netowrk overlay)
		kubelet container runtime (these two are daemons running on the worker node)

	kubectl get pods --all-namespaces -o wide # -o wide show u more info
	
	kubernetes is running within docker, installed within all master and worker nodes within kubernetes.
	kubernetes is basically manipulating docker under the hood.
	kubernetes uses an api server to manipulate docker
	
	
pods and containers:
	kubectl get namespaces
	default namespace is where everything gets put in by default
	kubectl create namespace podexample
	kubernetes is declarative
	create a yaml file with declarative statements mentioning what needs to happen within the pod and what all basically constitues a pod
	kubectl create -f ./pod-example.yaml
	
	pod-example.yaml:
	apiVersion: v1
	kind: Pod
	metadata:
	  name: examplepod
	  namespace: podexample
	spec:
	  volumes:
	  - name: html
		emptyDir: {}
	  containers:
	  - name: webcontainer
		image: nginx
		volumeMounts:
		- name: html
		  mountPath: /usr/share/nginx/html
	  - name: filecontainer
	    image: debian
		volumeMounts:
		- name: html
		  mountPath: /html
		  command: ["/bin/sh","-c"]
		  args:
		  - while true; do
		      date >> /html/index.html;
			  sleep 1;
			done
			
	Here, on the second container, all u are doing with args is setting a command that would run continously 
	so it doesnt run and exit automatically.
	as the second container doesnt have a command to run, it will shut down if nothing is mentioned.
	
	kubectl --namespace=podexample delete pod exmaplepod
	
	create the pod again
	kubectl --namesapce=podexample get pods
	kubectl --namesapce=podexample get pods - o wide # gives u more info about the pod like age, ip, node status etc.
	now, if u curl using the ip, u will see the nginx container reponse displaying contents of index.html, the same one onto which
	the other filecotnainer has been writing output of date comamnd to. this is possible because of the fact that we have 
	shared volume between teh two containers.
	## basically, ur pod is composed of two containers, and a shared volume. the two cotainers have the same ip info;
	the same ip is listed as pod's ip; u are packaging the working of a webcontainer and a filecotnainer into one entity called podexample pod.

	
networking:
	flannel network installed here
	flannel doesnt invoke network policies; there isnt post-configuration
	flanneld is run on each hsot
	stores state in etcd
	its a packet forwarder
	layer 3 ipv4 network

	ip route gives u all the routes setup within the host:
	root@htappd01286:/proc/sys/net/bridge $ ip route
	default via 10.62.212.1 dev eno16780032 proto static metric 100
	10.62.212.0/22 dev eno16780032 proto kernel scope link src 10.62.215.51 metric 100
	172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1
	172.18.0.0/16 dev docker_gwbridge proto kernel scope link src 172.18.0.1

	networking and ip address mgmt is provided by a network-plugin
	all kube-system pods except core dns are with ur host's network; 
	once flannel is setup with the host netwrok, it then assigns networking to core dns pods and assigns pod network ips to them.
	
	etcd is updated with ip information
	network plugin (flannel here) configures iptables on the nodes to setup routing that allows communciation between pods and nodes as well as with pods-on 
	other nodes within the cluster.
	
	
dns:
	dns pods have A records for all containers within the cluster; if u get into a pod and lookup resolv.conf file, u wil see
	that the cluster's dns server (ip of dns pod) is set as nameserver
	kubectl get pods
	kubectl exec -it <pod-name> /bin/bash
	$ cat /etc/resolv.conf shows nameserver as dns pod ip.
	this way u dont have to worry about conenctivity between any containers within one pod, or between other pods in a cluster.
	
replicasets:
	set of pods that all have the same label
	replicas-example.yaml
	apiVersion: apps/v1
	kind: ReplicaSet
	metadata:
	  name: frontend
	  labels:
	    app: nginx
		tier: frontend
	spec:
	  #modify replicas acc' to your case
	  replcias: 2
	  selector:
	    matchLabels:
		  tier: frontend
		matchExpressions:
		- { key: tier, operator: In, values: [frontend] }
	  template:
	    metadata:
		  labels:
		    app: nginx
			tier: frontend
		spec:
		  containers:
		  - name: nginx
		    image: image-name
		    ports:
		    - containerPort: 80
			
	
	kubectl -create -f ./replicas-example.yaml
	kubectl describe rs/frontend
	
	kubectl scale rs/frontend --replicas=4
	kubectl scale rs/frontend --replicas=1
	
	-- these statement are declarative;
	
services:
	put all pods behind a proxy and load balance the situation
	kube-proxy
		puts all pods that match a selector label and puts them behind a proxy
	kubectl create -f ./replica-example.yaml
	kubectl get pods
	kubectl describe <pod-name>
	./service-example.yaml:
	kind: Service
	apiVersion: v1
	metadata:
	  name: my-service
	spec:
	  selector:
	    app: nginx ## this selector we defined while creating replicasets; 
		#so this service uses this label to identify the pods it needs to grab and 
		#puts them behind a proxy with client side exposed port 32768.
		ports:
		- protocol: TCP
		  port: 32768
		  targetPort: 80
			
	kubectl create -f ./service-example.yaml
	kubectl describe service my-service
	 -- gives u details about the service like its name, namespace, labels, selector, ip, endpoint ips etc..
	 endpoint ips are obviously the endpoint ip for the pods within the cluster, mathcing the selector provided while creating the service.
	
	service -> replicasets -> pods -> containers and any shared volumes that make up one use case.
	
	u just need to either use the service's name or ip: 32768 to retrieve the service behind it
	it obfuscates the pods; u can scale the service up/down by scaling the replica set!!
	if a node dies and a pod within it, because kubernetes always is declarative it will create a new pod in another node for u to match the replica set definition.
	
	
deployments:
	deployments manage replica sets
	./deployexample.yaml:
	apiVersion: apps/v1
	kind: Deployment
	metadata:
	  name: example-deployment
	  labels:
	    app: nginx
	spec:
	  replicas: 2
	  selector:
		matchLabels:
		  app: nginx
	  template:
		metadata:
		  labels:
		    app: nginx
	    spec:
		  containers:
		  - name: nginx
			image: <image-name>:tag
			ports:
			- containerPort: 80
			
	kubectl create -f ./deployexample.yaml
	kubectl create -f ./ser	vice-example.yaml
	kubectl describe service my-service
	
	kubectl set image deployment.v1.apps/example-deployment nginx=<image-name>:v2
	update deplyoment image
	kubectl get deployment
	kubectl describe deployment
	
	gives u the ability to update services without having any pods.  -- always available types.
	
	##kubernetes essentials is the next stop
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
			
			
			
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	